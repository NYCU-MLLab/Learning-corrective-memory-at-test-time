#!/usr/bin/env python3
"""
æ¸¬è©¦ Longhorn æ¨¡å‹ç”Ÿæˆæ•ˆæœ
"""

import os
import sys
import torch
import argparse

# æ·»åŠ æ¨¡å‹è·¯å¾‘
sys.path.append('../../longhorn')
from models.longhorn import LonghornConfig, LonghornLM
from transformers import PreTrainedTokenizerFast

def load_longhorn_model(checkpoint_path, tokenizer_path, device):
    """è¼‰å…¥ Longhorn æ¨¡å‹"""
    print(f"ğŸ”„ è¼‰å…¥ Longhorn æ¨¡å‹: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model_config_dict = checkpoint['model_config']
    model_config = LonghornConfig(**model_config_dict)
    model = LonghornLM(model_config)
    model.load_state_dict(checkpoint['model'])
    model.to(device)
    model.eval()
    
    print(f"ğŸ”„ è¼‰å…¥ tokenizer: {tokenizer_path}")
    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    return model, tokenizer

def generate_with_longhorn(model, tokenizer, prompt, max_new_tokens=25, temperature=1.0, device="cuda"):
    """ä½¿ç”¨ Longhorn æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    # ç·¨ç¢¼è¼¸å…¥
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    
    print(f"ğŸ“ è¼¸å…¥: {prompt}")
    print(f"ğŸ”¢ è¼¸å…¥ tokens: {input_ids[0].tolist()}")
    
    # ç”Ÿæˆ
    with torch.no_grad():
        for i in range(max_new_tokens):
            # å‰å‘å‚³æ’­
            logits, _ = model(input_ids)
            
            # ç²å–æœ€å¾Œä¸€å€‹ token çš„ logits
            next_token_logits = logits[0, -1, :] / temperature
            
            # æ¡æ¨£
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # æ·»åŠ åˆ°åºåˆ—
            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)
            
            # è§£ç¢¼ä¸¦é¡¯ç¤º
            current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
            print(f"   Step {i+1}: {current_text}")
            
            # æª¢æŸ¥æ˜¯å¦ç”Ÿæˆäº† EOS
            if next_token.item() == tokenizer.eos_token_id:
                print(f"   âœ… ç”Ÿæˆäº† EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
                break
    
    return tokenizer.decode(input_ids[0], skip_special_tokens=True)

def main():
    parser = argparse.ArgumentParser(description="æ¸¬è©¦ Longhorn æ¨¡å‹ç”Ÿæˆ")
    parser.add_argument("--model", type=str, 
                       default="../../longhorn/results/slim_results/LH_135.1M_8000iter_7.16_best/Slim_2.1_block1024_longhorn_best.pt",
                       help="Longhorn æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--tokenizer", type=str, 
                       default="../../tokenizer/slim_tokenizer",
                       help="Tokenizer è·¯å¾‘")
    parser.add_argument("--device", type=str, default="cuda", help="è¨­å‚™")
    
    args = parser.parse_args()
    
    if args.device == "cuda" and not torch.cuda.is_available():
        args.device = "cpu"
    
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {args.device}")
    
    try:
        # è¼‰å…¥æ¨¡å‹å’Œ tokenizer
        model, tokenizer = load_longhorn_model(args.model, args.tokenizer, args.device)
        
        # æ¸¬è©¦ prompts
        test_prompts = [
            "Hello",
            "The story begins",
            "Once upon a time",
            "In the year 2024",
            "The scientist discovered"
        ]
        
        print("\n" + "="*60)
        print("ğŸ§ª Longhorn æ¨¡å‹ç”Ÿæˆæ¸¬è©¦")
        print("="*60)
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n{'='*50}")
            print(f"ğŸ§ª æ¸¬è©¦ {i}: {prompt}")
            print(f"{'='*50}")
            
            result = generate_with_longhorn(model, tokenizer, prompt, max_new_tokens=25, temperature=1.0, device=args.device)
            print(f"\nâœ… æœ€çµ‚çµæœ: {result}")
            print(f"{'='*50}")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 