#!/usr/bin/env python3
"""
ç°¡å–®çš„ PreCo æ¨¡å‹ç”Ÿæˆæ¸¬è©¦
ä½¿ç”¨æ›´ä¿å®ˆçš„åƒæ•¸ä¾†æ¸¬è©¦åŸºæœ¬ç”Ÿæˆèƒ½åŠ›
"""

import os
import sys
import torch
import argparse

# æ·»åŠ æ¨¡å‹è·¯å¾‘
sys.path.append('../../longhorn')
from models.PreCo import PreCoNewConfig, PreCoNewModel
from transformers import PreTrainedTokenizerFast

def load_model_and_tokenizer(model_path, tokenizer_path, device):
    """è¼‰å…¥æ¨¡å‹å’Œ tokenizer"""
    print(f"ğŸ”„ è¼‰å…¥æ¨¡å‹: {model_path}")
    
    checkpoint = torch.load(model_path, map_location=device)
    model_config_dict = checkpoint['model_config']
    model_config = PreCoNewConfig(**model_config_dict)
    model = PreCoNewModel(model_config)
    model.load_state_dict(checkpoint['model'])
    model.to(device)
    model.eval()
    
    print(f"ğŸ”„ è¼‰å…¥ tokenizer: {tokenizer_path}")
    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    return model, tokenizer

def simple_generate(model, tokenizer, prompt, max_new_tokens=20, temperature=1.0, device="cuda"):
    """ç°¡å–®çš„ç”Ÿæˆå‡½æ•¸"""
    # ç·¨ç¢¼è¼¸å…¥
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    
    print(f"ğŸ“ è¼¸å…¥: {prompt}")
    print(f"ğŸ”¢ è¼¸å…¥ tokens: {input_ids[0].tolist()}")
    
    # ç”Ÿæˆ
    with torch.no_grad():
        for i in range(max_new_tokens):
            # å‰å‘å‚³æ’­
            outputs = model(input_ids, ttt_lr_mult=0.1)
            
            if isinstance(outputs, tuple):
                logits = outputs[0]
            else:
                logits = outputs
            
            # ç²å–æœ€å¾Œä¸€å€‹ token çš„ logits
            next_token_logits = logits[0, -1, :] / temperature
            
            # ç°¡å–®çš„æ¡æ¨£
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # æ·»åŠ åˆ°åºåˆ—
            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)
            
            # è§£ç¢¼ä¸¦é¡¯ç¤º
            current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
            print(f"   Step {i+1}: {current_text}")
            
            # æª¢æŸ¥æ˜¯å¦ç”Ÿæˆäº† EOS
            if next_token.item() == tokenizer.eos_token_id:
                break
    
    return tokenizer.decode(input_ids[0], skip_special_tokens=True)

def main():
    parser = argparse.ArgumentParser(description="ç°¡å–®çš„ PreCo ç”Ÿæˆæ¸¬è©¦")
    parser.add_argument("--model", type=str, 
                       default="../../longhorn/results/writingprompt_finetuned_full/writingprompt_best.pt",
                       help="æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--tokenizer", type=str, 
                       default="../../tokenizer/slim_tokenizer",
                       help="Tokenizer è·¯å¾‘")
    parser.add_argument("--device", type=str, default="cuda", help="è¨­å‚™")
    
    args = parser.parse_args()
    
    if args.device == "cuda" and not torch.cuda.is_available():
        args.device = "cpu"
    
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {args.device}")
    
    try:
        # è¼‰å…¥æ¨¡å‹å’Œ tokenizer
        model, tokenizer = load_model_and_tokenizer(args.model, args.tokenizer, args.device)
        
        # æ¸¬è©¦ prompts
        test_prompts = [
            "Hello",
            "The story begins",
            "Once upon a time",
            "In the year 2024",
            "The scientist discovered"
        ]
        
        print("\n" + "="*60)
        print("ğŸ§ª ç°¡å–®ç”Ÿæˆæ¸¬è©¦")
        print("="*60)
        
        for prompt in test_prompts:
            print(f"\n{'='*50}")
            result = simple_generate(model, tokenizer, prompt, max_new_tokens=10, temperature=1.0, device=args.device)
            print(f"\nâœ… æœ€çµ‚çµæœ: {result}")
            print(f"{'='*50}")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 