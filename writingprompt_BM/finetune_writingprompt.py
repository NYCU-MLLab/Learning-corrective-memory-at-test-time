#!/usr/bin/env python3
"""
PreCo æ¨¡å‹ WritingPrompt å¾®èª¿è…³æœ¬
"""

import os
import sys
import torch
import torch.nn.functional as F
import json
import time
import argparse
from typing import List, Dict
import random

# æ·»åŠ æ¨¡å‹è·¯å¾‘
sys.path.append('../../longhorn')
from models.PreCo import PreCoNewConfig, PreCoNewModel
from transformers import PreTrainedTokenizerFast
from tokenizers import Tokenizer

def load_preco_model(checkpoint_path: str, device: str = "cuda") -> tuple:
    """è¼‰å…¥é è¨“ç·´çš„ PreCo æ¨¡å‹"""
    print(f"ğŸ”„ è¼‰å…¥é è¨“ç·´æ¨¡å‹: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model_config_dict = checkpoint['model_config']
    model_config = PreCoNewConfig(**model_config_dict)
    model = PreCoNewModel(model_config)
    model.load_state_dict(checkpoint['model'])
    model.to(device)
    
    print("âœ… é è¨“ç·´æ¨¡å‹è¼‰å…¥æˆåŠŸ!")
    return model, model_config

def load_tokenizer(tokenizer_path: str = "../tokenizer/slim_tokenizer"):
    """è¼‰å…¥ tokenizer"""
    print(f"ğŸ”„ è¼‰å…¥ tokenizer: {tokenizer_path}")
    
    try:
        # ä½¿ç”¨ PreTrainedTokenizerFast è¼‰å…¥
        tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)
        print("âœ… Tokenizer è¼‰å…¥æˆåŠŸ!")
    except Exception as e:
        print(f"âš ï¸  Tokenizer è¼‰å…¥å¤±æ•—: {e}")
        # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ AutoTokenizer ä½œç‚ºå‚™ç”¨
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
    
    # è¨­ç½® EOS token ID
    if tokenizer.eos_token_id is None:
        tokenizer.eos_token_id = tokenizer.vocab_size - 1
    
    # è¨­ç½® padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        
    return tokenizer

def load_writingprompt_data(file_path: str) -> List[Dict]:
    """è¼‰å…¥ WritingPrompt æ•¸æ“šï¼Œä½¿ç”¨æ˜ç¢ºçš„åˆ†éš”ç¬¦æ ¼å¼"""
    data_list = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            # æª¢æŸ¥æ•¸æ“šæ ¼å¼
            if 'text' in data:
                # å·²ç¶“è™•ç†éçš„æ ¼å¼
                data_list.append({
                    'text': data['text'],
                    'prompt_end': len(data['text'])  # æ•´å€‹æ–‡æœ¬éƒ½æ˜¯ target
                })
            elif 'prompt' in data and 'target' in data:
                # åŸå§‹ WritingPrompt æ ¼å¼ - ä½¿ç”¨æ˜ç¢ºåˆ†éš”ç¬¦
                prompt = data['prompt']
                target = data['target']
                # ä½¿ç”¨æ˜ç¢ºçš„åˆ†éš”ç¬¦æ ¼å¼
                full_text = f"Prompt: {prompt}\nResponse: {target}"
                prompt_end = len(f"Prompt: {prompt}\nResponse: ")
                data_list.append({
                    'text': full_text,
                    'prompt_end': prompt_end
                })
            else:
                print(f"âš ï¸  æœªçŸ¥æ•¸æ“šæ ¼å¼: {list(data.keys())}")
                continue
    return data_list

def create_training_batch(data_list: List[Dict], tokenizer, block_size: int = 512, batch_size: int = 4) -> tuple:
    """å‰µå»ºè¨“ç·´æ‰¹æ¬¡ï¼Œåªå° target éƒ¨åˆ†è¨ˆç®— loss"""
    # éš¨æ©Ÿé¸æ“‡æ¨£æœ¬
    batch_data = random.sample(data_list, min(batch_size, len(data_list)))
    
    # ç·¨ç¢¼æ–‡æœ¬
    input_ids_batch = []
    targets_batch = []
    prompt_end_positions = []
    
    for data in batch_data:
        text = data['text']
        prompt_end = data['prompt_end']
        
        # ç·¨ç¢¼å®Œæ•´æ–‡æœ¬
        encoded = tokenizer.encode(text, max_length=block_size, truncation=True)
        input_ids_batch.append(encoded)
        
        # å‰µå»º targetsï¼Œåªå° target éƒ¨åˆ†è¨ˆç®— loss
        targets = [-100] * len(encoded)  # -100 è¡¨ç¤ºå¿½ç•¥è©²ä½ç½®çš„ loss
        
        # æ‰¾åˆ° prompt çµæŸä½ç½®å°æ‡‰çš„ token ä½ç½®
        prompt_text = text[:prompt_end]
        prompt_tokens = tokenizer.encode(prompt_text, max_length=block_size, truncation=True)
        target_start_pos = len(prompt_tokens)
        
        # å¾ target é–‹å§‹ä½ç½®è¨­ç½® targets
        for i in range(target_start_pos, len(encoded)):
            targets[i] = encoded[i]
        
        targets_batch.append(targets)
        prompt_end_positions.append(target_start_pos)
    
    # å¡«å……åˆ°ç›¸åŒé•·åº¦
    max_len = max(len(seq) for seq in input_ids_batch)
    padded_input_ids = []
    padded_targets = []
    
    for input_ids, targets in zip(input_ids_batch, targets_batch):
        # å¡«å…… input_ids
        padded_input = input_ids + [tokenizer.pad_token_id] * (max_len - len(input_ids))
        padded_input_ids.append(padded_input)
        
        # å¡«å…… targets
        padded_target = targets + [-100] * (max_len - len(targets))
        padded_targets.append(padded_target)
    
    # è½‰æ›ç‚º tensor
    input_ids = torch.tensor(padded_input_ids)
    targets = torch.tensor(padded_targets)
    
    return input_ids, targets

def train_epoch(model, tokenizer, train_data, optimizer, device, 
                block_size=512, batch_size=4, num_batches=100):
    """è¨“ç·´ä¸€å€‹ epoch"""
    model.train()
    total_loss = 0
    
    for batch_idx in range(num_batches):
        # å‰µå»ºæ‰¹æ¬¡
        input_ids, targets = create_training_batch(train_data, tokenizer, block_size, batch_size)
        input_ids = input_ids.to(device)
        targets = targets.to(device)
        
        # å‰å‘å‚³æ’­
        optimizer.zero_grad()
        logits, loss_dict = model(input_ids, targets=targets, ttt_lr_mult=0.3)
        loss = loss_dict['total_loss']
        
        # åå‘å‚³æ’­
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if batch_idx % 10 == 0:
            print(f"  Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}")
    
    return total_loss / num_batches

def evaluate_model(model, tokenizer, val_data, device, 
                  block_size=512, batch_size=4, num_batches=20):
    """è©•ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for batch_idx in range(num_batches):
            # å‰µå»ºæ‰¹æ¬¡
            input_ids, targets = create_training_batch(val_data, tokenizer, block_size, batch_size)
            input_ids = input_ids.to(device)
            targets = targets.to(device)
            
            # å‰å‘å‚³æ’­
            logits, loss_dict = model(input_ids, targets=targets, ttt_lr_mult=0.3)
            loss = loss_dict['total_loss']
            
            total_loss += loss.item()
    
    return total_loss / num_batches

def save_checkpoint(model, optimizer, epoch, loss, save_path: str):
    """ä¿å­˜æª¢æŸ¥é»"""
    checkpoint = {
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'epoch': epoch,
        'loss': loss,
        'model_config': {
            'vocab_size': model.config.vocab_size,
            'd_model': model.config.d_model,
            'n_layer': model.config.n_layer,
            'd_state': model.config.d_state,
            'd_conv': model.config.d_conv,
            'expand': model.config.expand,
            'ttt_num_heads': model.config.ttt_num_heads,
            'ttt_num_layers': model.config.ttt_num_layers,
            'dropout': model.config.dropout,
        }
    }
    
    torch.save(checkpoint, save_path)
    print(f"âœ… æª¢æŸ¥é»å·²ä¿å­˜: {save_path}")

def main():
    parser = argparse.ArgumentParser(description="PreCo WritingPrompt å¾®èª¿")
    parser.add_argument("--pretrained_model", type=str, 
                       default="../../longhorn/results/slim_results/preco_136M_8000iter_7.23_bestnoTB/Slim_2.1_block1024_preco_best.pt",
                       help="é è¨“ç·´æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--input_data", type=str, 
                       default="../../dataset/writingprompts/filtered_wp_data.jsonl",
                       help="è¼¸å…¥æ•¸æ“šè·¯å¾‘")
    parser.add_argument("--tokenizer", type=str, 
                       default="../../tokenizer/slim_tokenizer",
                       help="Tokenizer è·¯å¾‘")
    parser.add_argument("--output_dir", type=str, 
                       default="../../longhorn/results/writingprompt_finetuned_1000",
                       help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--epochs", type=int, default=1, help="è¨“ç·´è¼ªæ•¸")
    parser.add_argument("--batch_size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--block_size", type=int, default=1024, help="åºåˆ—é•·åº¦")
    parser.add_argument("--learning_rate", type=float, default=1e-5, help="å­¸ç¿’ç‡")
    parser.add_argument("--device", type=str, default="cuda", help="è¨­å‚™")
    parser.add_argument("--conservative_mode", action="store_true", 
                       help="ä¿å®ˆæ¨¡å¼ï¼šä½¿ç”¨æ›´å°‘çš„è³‡æ–™å’Œæ›´ä½çš„å­¸ç¿’ç‡")
    parser.add_argument("--max_samples", type=int, default=2000, help="æœ€å¤§è¨“ç·´æ¨£æœ¬æ•¸")
    parser.add_argument("--val_ratio", type=float, default=0.2, help="é©—è­‰é›†æ¯”ä¾‹")
    parser.add_argument("--train_samples", type=int, default=1600, help="è¨“ç·´æ¨£æœ¬æ•¸")
    parser.add_argument("--val_samples", type=int, default=400, help="é©—è­‰æ¨£æœ¬æ•¸")
    
    args = parser.parse_args()
    
    # ä¿å®ˆæ¨¡å¼è¨­ç½®
    if args.conservative_mode:
        print("ğŸ›¡ï¸  å•Ÿç”¨ä¿å®ˆæ¨¡å¼ï¼šé˜²æ­¢éæ“¬åˆ")
        args.learning_rate = 5e-6  # æ›´ä½çš„å­¸ç¿’ç‡
        args.max_samples = 1000    # æ›´å°‘çš„è³‡æ–™
        args.train_samples = 800   # 800 è¨“ç·´ + 200 é©—è­‰
        args.val_samples = 200
        args.epochs = 1            # åªè¨“ç·´ 1 è¼ª
        args.batch_size = 1        # æ›´å°çš„æ‰¹æ¬¡
    
    # æª¢æŸ¥è¨­å‚™
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        args.device = "cpu"
    
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {args.device}")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(args.output_dir, exist_ok=True)
    
    try:
        # è¼‰å…¥æ¨¡å‹å’Œ tokenizer
        model, model_config = load_preco_model(args.pretrained_model, args.device)
        tokenizer = load_tokenizer(args.tokenizer)
        
        # è¼‰å…¥ä¸¦è™•ç†æ•¸æ“š
        print(f"ğŸ“– è¼‰å…¥åŸå§‹æ•¸æ“š: {args.input_data}")
        all_data = load_writingprompt_data(args.input_data)
        print(f"âœ… è¼‰å…¥äº† {len(all_data)} å€‹æ¨£æœ¬")
        
        # é™åˆ¶æ¨£æœ¬æ•¸é‡ä»¥é¿å…éæ“¬åˆ
        if len(all_data) > args.max_samples:
            print(f"ğŸ”„ é™åˆ¶æ¨£æœ¬æ•¸é‡ç‚º {args.max_samples}")
            all_data = all_data[:args.max_samples]
        
        # å‰µå»ºè¨“ç·´/é©—è­‰åˆ†å‰²
        print(f"ğŸ”„ å‰µå»ºè¨“ç·´/é©—è­‰åˆ†å‰² (é©—è­‰æ¯”ä¾‹: {args.val_ratio})")
        
        # éš¨æ©Ÿæ‰“äº‚è³‡æ–™
        random.shuffle(all_data)
        
        # å¦‚æœæŒ‡å®šäº†å…·é«”çš„æ¨£æœ¬æ•¸ï¼Œä½¿ç”¨æŒ‡å®šçš„æ•¸é‡
        if args.train_samples > 0 and args.val_samples > 0:
            total_requested = args.train_samples + args.val_samples
            if total_requested > len(all_data):
                print(f"âš ï¸  è«‹æ±‚çš„æ¨£æœ¬æ•¸ ({total_requested}) è¶…éå¯ç”¨æ¨£æœ¬æ•¸ ({len(all_data)})")
                print(f"   ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ¨£æœ¬")
                train_data = all_data[args.val_samples:]
                val_data = all_data[:args.val_samples]
            else:
                train_data = all_data[args.val_samples:args.val_samples + args.train_samples]
                val_data = all_data[:args.val_samples]
        else:
            # ä½¿ç”¨æ¯”ä¾‹åˆ‡å‰²
            val_size = int(len(all_data) * args.val_ratio)
            train_data = all_data[val_size:]
            val_data = all_data[:val_size]
        
        print(f"ğŸ“Š æ•¸æ“šçµ±è¨ˆ:")
        print(f"  - ç¸½æ¨£æœ¬æ•¸: {len(all_data)}")
        print(f"  - è¨“ç·´æ¨£æœ¬: {len(train_data)}")
        print(f"  - é©—è­‰æ¨£æœ¬: {len(val_data)}")
        
        # è¨­ç½®å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨
        optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=0.1)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
        
        # æ—©åœæ©Ÿåˆ¶
        best_val_loss = float('inf')
        patience = 3
        patience_counter = 0
        early_stop = False
        
        # è¨“ç·´å¾ªç’°
        print(f"\nğŸš€ é–‹å§‹å¾®èª¿è¨“ç·´...")
        print(f"  - è¨“ç·´è¼ªæ•¸: {args.epochs}")
        print(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        print(f"  - å­¸ç¿’ç‡: {args.learning_rate}")
        print(f"  - åºåˆ—é•·åº¦: {args.block_size}")
        
        for epoch in range(args.epochs):
            print(f"\nğŸ“… Epoch {epoch + 1}/{args.epochs}")
            print("-" * 40)
            
            # è¨ˆç®—æ¯è¼ªçš„æ‰¹æ¬¡æ•¸ï¼ˆåŸºæ–¼æ•¸æ“šé›†å¤§å°ï¼‰
            num_train_batches = min(200, len(train_data) // args.batch_size)  # æœ€å¤š200å€‹æ‰¹æ¬¡
            num_val_batches = min(50, len(val_data) // args.batch_size)       # æœ€å¤š50å€‹æ‰¹æ¬¡
            
            print(f"  ğŸ“Š æ‰¹æ¬¡è¨­ç½®:")
            print(f"    - è¨“ç·´æ‰¹æ¬¡: {num_train_batches}")
            print(f"    - é©—è­‰æ‰¹æ¬¡: {num_val_batches}")
            
            # è¨“ç·´
            start_time = time.time()
            train_loss = train_epoch(
                model, tokenizer, train_data, optimizer, args.device,
                args.block_size, args.batch_size, num_batches=num_train_batches
            )
            train_time = time.time() - start_time
            
            # é©—è­‰
            start_time = time.time()
            val_loss = evaluate_model(
                model, tokenizer, val_data, args.device,
                args.block_size, args.batch_size, num_batches=num_val_batches
            )
            val_time = time.time() - start_time
            
            print(f"  ğŸ“Š çµæœ:")
            print(f"    - è¨“ç·´æå¤±: {train_loss:.4f} (æ™‚é–“: {train_time:.1f}s)")
            print(f"    - é©—è­‰æå¤±: {val_loss:.4f} (æ™‚é–“: {val_time:.1f}s)")
            
            # æª¢æŸ¥éæ“¬åˆ
            train_val_gap = train_loss - val_loss
            print(f"    - è¨“ç·´/é©—è­‰å·®è·: {train_val_gap:.4f}")
            
            if train_val_gap < -0.1:  # é©—è­‰æå¤±æ¯”è¨“ç·´æå¤±ä½å¾ˆå¤šï¼Œå¯èƒ½éæ“¬åˆ
                print(f"    âš ï¸  è­¦å‘Šï¼šå¯èƒ½éæ“¬åˆ (å·®è·: {train_val_gap:.4f})")
            
            # æ—©åœæ©Ÿåˆ¶
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                save_path = os.path.join(args.output_dir, f"writingprompt_best.pt")
                save_checkpoint(model, optimizer, epoch, val_loss, save_path)
                print(f"    âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
            else:
                patience_counter += 1
                print(f"    ğŸ“‰ é©—è­‰æå¤±æœªæ”¹å–„ ({patience_counter}/{patience})")
                
                if patience_counter >= patience:
                    print(f"    ğŸ›‘ æ—©åœï¼šé©—è­‰æå¤±é€£çºŒ {patience} æ¬¡æœªæ”¹å–„")
                    early_stop = True
                    break
            
            # æ›´æ–°å­¸ç¿’ç‡
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
            
            print(f"    - ç•¶å‰å­¸ç¿’ç‡: {current_lr:.2e}")
            
            # ä¿å­˜å®šæœŸæª¢æŸ¥é»
            if (epoch + 1) % 1 == 0:
                save_path = os.path.join(args.output_dir, f"writingprompt_epoch_{epoch + 1}.pt")
                save_checkpoint(model, optimizer, epoch, val_loss, save_path)
        
        print(f"\nğŸ‰ å¾®èª¿å®Œæˆ!")
        print(f"  - æœ€ä½³é©—è­‰æå¤±: {best_val_loss:.4f}")
        print(f"  - æ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

def format_prompt_for_generation(prompt: str) -> str:
    """æ ¼å¼åŒ– prompt ç”¨æ–¼ç”Ÿæˆï¼Œèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´"""
    return f"Prompt: {prompt}\nResponse:"

if __name__ == "__main__":
    main() 