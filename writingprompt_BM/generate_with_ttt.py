#!/usr/bin/env python3
"""
ä½¿ç”¨ TTT æ¨¡å‹ç”Ÿæˆ WritingPrompt é æ¸¬çµæœ
"""

import json
import torch
import sys
import os

# æ·»åŠ æ¨¡å‹è·¯å¾‘
sys.path.append('../../longhorn')
from models.ttt import TTTConfig, TTT
from transformers import PreTrainedTokenizerFast

def load_ttt_model(model_path, device='cuda'):
    """è¼‰å…¥ TTT æ¨¡å‹"""
    print(f"è¼‰å…¥ TTT æ¨¡å‹å¾: {model_path}")
    
    # è¼‰å…¥æ¬Šé‡
    checkpoint = torch.load(model_path, map_location=device)
    print(f"Checkpoint keys: {checkpoint.keys()}")
    
    # å¾ checkpoint ä¸­ç²å–æ­£ç¢ºçš„é…ç½®
    if 'model_config' in checkpoint:
        model_config = checkpoint['model_config']
        print(f"ğŸ“‹ Using model config from checkpoint: {model_config}")
        
        config = TTTConfig(
            vocab_size=model_config.get('vocab_size', 50257),
            hidden_size=model_config.get('hidden_size', 512),
            num_hidden_layers=model_config.get('num_hidden_layers', 12),
            num_attention_heads=model_config.get('num_attention_heads', 8),
            max_position_embeddings=model_config.get('max_position_embeddings', 1024),
            ttt_base_lr=model_config.get('ttt_base_lr', 1.0),
            mini_batch_size=model_config.get('mini_batch_size', 8),
            use_gate=model_config.get('use_gate', True),
            share_qk=model_config.get('share_qk', False),
            ttt_layer_type=model_config.get('ttt_layer_type', 'linear'),
            pre_conv=model_config.get('pre_conv', True),
            conv_kernel=model_config.get('conv_kernel', 2),
            scan_checkpoint_group_size=model_config.get('scan_checkpoint_group_size', 1),
            mlp_ratio=model_config.get('mlp_ratio', 14),
            pad_token_id=model_config.get('pad_token_id', 0),
            bos_token_id=model_config.get('bos_token_id', 2),
            eos_token_id=model_config.get('eos_token_id', 3),
            dropout=model_config.get('dropout', 0.1),
        )
    else:
        # ä½¿ç”¨é è¨­é…ç½®
        print("ğŸ“‹ Using default TTT config")
        config = TTTConfig(
            vocab_size=50257,
            hidden_size=512,
            num_hidden_layers=12,
            num_attention_heads=8,
            max_position_embeddings=1024,
            ttt_base_lr=1.0,
            mini_batch_size=8,
            use_gate=True,
            share_qk=False,
            ttt_layer_type="linear",
            pre_conv=True,
            conv_kernel=2,
            scan_checkpoint_group_size=1,
            mlp_ratio=14,
            pad_token_id=0,
            bos_token_id=2,
            eos_token_id=3,
            dropout=0.1,
        )
    
    print(f"ä½¿ç”¨ TTT é…ç½®: vocab_size={config.vocab_size}, hidden_size={config.hidden_size}, num_layers={config.num_hidden_layers}")
    
    # å»ºç«‹æ¨¡å‹
    model = TTT(config)
    
    # è¼‰å…¥æ¬Šé‡
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    elif 'model' in checkpoint:
        state_dict = checkpoint['model']
    else:
        state_dict = checkpoint
    
    model.load_state_dict(state_dict, strict=False)
    
    model.to(device)
    model.eval()
    print("âœ… TTT æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    return model

def load_tokenizer(tokenizer_path):
    """è¼‰å…¥ tokenizer"""
    print(f"è¼‰å…¥ tokenizer å¾: {tokenizer_path}")
    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    print("âœ… Tokenizer è¼‰å…¥æˆåŠŸ")
    return tokenizer

def generate_response(model, tokenizer, prompt, max_length=512, temperature=0.8, top_p=0.9, device='cuda'):
    """ä½¿ç”¨ TTT æ¨¡å‹ç”Ÿæˆå›ç­”"""
    # ç·¨ç¢¼è¼¸å…¥
    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)
    input_length = inputs.shape[1]
    
    # æ‰‹å‹•å¯¦ç¾ç”Ÿæˆé‚è¼¯
    generated = inputs.clone()
    
    with torch.no_grad():
        for step in range(max_length - input_length):
            # TTT æ¨¡å‹å‰å‘å‚³æ’­ - ä½¿ç”¨ä¸åŒçš„è¼¸å‡ºæ ¼å¼
            try:
                logits, _, _ = model(
                    input_ids=generated, 
                    labels=None,
                    loss_masks=None,
                    ttt_lr_mult=1.0  # æ¨ç†æ™‚ä½¿ç”¨æ¨™æº– TTT å­¸ç¿’ç‡
                )
            except Exception as e:
                print(f"TTT å‰å‘å‚³æ’­éŒ¯èª¤: {e}")
                break
            
            # åªå–æœ€å¾Œä¸€å€‹ token çš„ logits
            next_token_logits = logits[0, -1, :] / temperature
            
            # é¿å…ç”Ÿæˆç‰¹æ®Šå­—ç¬¦
            for token_id in [179, 374, tokenizer.pad_token_id]:  # æ›è¡Œç¬¦ã€tabã€pad token
                if token_id is not None:
                    next_token_logits[token_id] = float('-inf')
            
            # æ‡‰ç”¨ nucleus sampling (top-p)
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                
                # ç§»é™¤ç´¯ç©æ¦‚ç‡è¶…é top_p çš„ token
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = 0
                
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # æ¡æ¨£ä¸‹ä¸€å€‹ token
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)
            
            # æª¢æŸ¥æ˜¯å¦ç”Ÿæˆäº†çµæŸç¬¦è™Ÿ
            if next_token.item() == tokenizer.eos_token_id:
                break
            
            # æª¢æŸ¥æ˜¯å¦é‡è¤‡å¤ªå¤šï¼ˆæ”¹é€²çš„é‡è¤‡æª¢æ¸¬ï¼‰
            if step > 20:
                last_20 = generated[0, -20:].tolist()
                unique_tokens = len(set(last_20))
                if unique_tokens <= 5:  # å¦‚æœæœ€å¾Œ20å€‹tokenä¸­åªæœ‰5å€‹ä¸åŒçš„token
                    break
                
                # æª¢æŸ¥æ˜¯å¦æœ‰é€£çºŒé‡è¤‡
                if step > 5:
                    last_5 = generated[0, -5:].tolist()
                    if len(set(last_5)) == 1:  # å¦‚æœæœ€å¾Œ5å€‹tokenéƒ½ç›¸åŒ
                        break
    
    # è§£ç¢¼è¼¸å‡ºï¼ˆåªå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    generated_text = tokenizer.decode(generated[0][input_length:], skip_special_tokens=True)
    return generated_text.strip()

def load_test_data(file_path):
    """è¼‰å…¥æ¸¬è©¦è³‡æ–™"""
    prompts = []
    targets = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            prompts.append(data['prompt'])
            targets.append(data['target'])
    return prompts, targets

def save_predictions(prompts, predictions, targets, filename='ttt_predictions.json'):
    """ä¿å­˜é æ¸¬çµæœ"""
    data = []
    for i, (prompt, pred, target) in enumerate(zip(prompts, predictions, targets)):
        data.append({
            'id': i,
            'prompt': prompt,
            'prediction': pred,
            'target': target
        })
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… TTT é æ¸¬çµæœå·²ä¿å­˜åˆ° {filename}")

def main():
    # è¨­å®š
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # TTT æ¨¡å‹å’Œ tokenizer è·¯å¾‘
    # è‡ªå‹•å°‹æ‰¾æœ€æ–°çš„ TTT å¾®èª¿çµæœ
    import glob
    finetune_dirs = glob.glob("../../longhorn/finetune_results/ttt_finetune_*")
    if finetune_dirs:
        # æŒ‰æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
        finetune_dirs.sort()
        latest_dir = finetune_dirs[-1]
        model_path = os.path.join(latest_dir, "best_checkpoint.pt")
        print(f"ğŸ” Found latest TTT finetune directory: {latest_dir}")
    else:
        print("âš ï¸  No TTT finetune results found, using default path")
        model_path = "../../longhorn/results/slim_results/ttt_137.9M_8000iter_7.17_best/Slim_2.1_block1024_ttt_best.pt"
    
    tokenizer_path = "../../tokenizer/slim_tokenizer"
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ‰¾ä¸åˆ° TTT æ¨¡å‹æª”æ¡ˆ: {model_path}")
        print("è«‹ä¿®æ”¹ model_path ç‚ºæ‚¨çš„å¯¦éš› TTT æ¨¡å‹è·¯å¾‘")
        return
    
    if not os.path.exists(tokenizer_path):
        print(f"âŒ æ‰¾ä¸åˆ° tokenizer è·¯å¾‘: {tokenizer_path}")
        print("è«‹ä¿®æ”¹ tokenizer_path ç‚ºæ‚¨çš„å¯¦éš› tokenizer è·¯å¾‘")
        return
    
    # è¼‰å…¥æ¨¡å‹å’Œ tokenizer
    model = load_ttt_model(model_path, device)
    tokenizer = load_tokenizer(tokenizer_path)
    
    # è¼‰å…¥æ¸¬è©¦è³‡æ–™
    print("è¼‰å…¥æ¸¬è©¦è³‡æ–™...")
    prompts, targets = load_test_data('test_50.jsonl')
    print(f"è¼‰å…¥äº† {len(prompts)} å€‹æ¸¬è©¦æ¨£æœ¬")
    
    # ç”Ÿæˆé æ¸¬
    predictions = []
    print("\né–‹å§‹ä½¿ç”¨ TTT æ¨¡å‹ç”Ÿæˆé æ¸¬...")
    
    for i, prompt in enumerate(prompts):
        print(f"TTT ç”Ÿæˆç¬¬ {i+1}/50 å€‹å›ç­”...")
        try:
            response = generate_response(model, tokenizer, prompt, device=device)
            predictions.append(response)
            print(f"  å®Œæˆ: {response[:100]}...")
        except Exception as e:
            print(f"  éŒ¯èª¤: {e}")
            predictions.append("ç”Ÿæˆå¤±æ•—")
    
    # ä¿å­˜çµæœ
    save_predictions(prompts, predictions, targets)
    
    # é¡¯ç¤ºå‰å¹¾å€‹çµæœ
    print("\nå‰ 3 å€‹ TTT ç”Ÿæˆçµæœ:")
    for i in range(min(3, len(predictions))):
        print(f"\næ¨£æœ¬ {i+1}:")
        print(f"Prompt: {prompts[i]}")
        print(f"TTT Generated: {predictions[i][:200]}...")
        print(f"Target: {targets[i][:200]}...")

if __name__ == "__main__":
    main() 