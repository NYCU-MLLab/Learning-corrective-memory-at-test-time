"""
line 59, 467
This training script can be run both on a single gpu in debug mode,
and also in a larger training run with distributed data parallel (ddp).

To run on a single GPU, example:
$ python train.py --batch_size=32 --compile=False

To run with DDP on 4 gpus on 1 node, example:
$ torchrun --standalone --nproc_per_node=4 train.py

To run with DDP on 4 gpus across 2 nodes, example:
- Run on the first (master) node with example IP 123.456.123.456:
$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
- Run on the worker node:
$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)
"""
import os
import time
import inspect
import math
import pickle
from contextlib import nullcontext
import numpy as np
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
from torch.nn import Linear as nnLinear
import torch.nn as nn
import torch.nn.functional as F

from mamba_ssm.models.config_mamba import MambaConfig
from models import *
from models.ttt import TTTLinear, TTTConfig, TTTForCausalLM
from models.PreCo import PreCoNewConfig, PreCoNewModel
from models.preco_nogain import PreCoNoGainConfig, PreCoNoGainModel
import wandb

# -----------------------------------------------------------------------------
# default config values designed to train a gpt2 (124M) on OpenWebText
# I/O
# master_seed = 1337
# out_dir = 'results'
# eval_interval = 2000
# log_interval = 1
# eval_iters = 200
# eval_only = False # if True, script exits right after the first eval
# always_save_checkpoint = False # if True, always save a checkpoint after each eval
master_seed = 1337
out_dir = 'results/slim_results'
eval_interval = 500
log_interval = 10
eval_iters = 50
eval_only = False
always_save_checkpoint = False

# model
model_name = "preco" # llama, gla, rwkv, retnet, mamba, longhorn, ttt, preco, preco_nogain

# data
# dataset = 'openwebtext'
# dataset = 'ED'
dataset = 'Slim'
do_eval = True
# gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes
gradient_accumulation_steps = 16
# batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size
# block_size = 1024
batch_size = 4 #mambaéœ€è¦è¼ƒå¤§çš„batch size #longhorn:4
block_size = 256 #é è¨­å€¼æœƒè¢«run.sh çš„ block_size è¦†è“‹

# åˆ†æ”¯æå¤±ï¼ˆPreCoï¼‰
use_branch_losses = False  # åƒ…ç”¨ CE ç«¯åˆ°ç«¯è¨“ç·´å…©åˆ†æ”¯ï¼ˆå»ºè­°ï¼‰
branch_longhorn_weight = 1.0
branch_ttt_weight = 1.0

# model - 130M TTTé…ç½®å„ªåŒ–
n_head = 8    # å¾6é ­å¢åŠ åˆ°8é ­ (768/8=96 head_dim)
n_embd = 512  # å¾1024ç¶­æ¸›å°‘åˆ°768ç¶­ (å¹³è¡¡åƒæ•¸åˆ†é…)
dropout = 0.2
n_layer = 12  # TTT å±¤æ•¸é…ç½® (å¾15å±¤æ¸›å°‘åˆ°12å±¤)
# åŸé…ç½® (568M): 32å±¤Ã—1024ç¶­Ã—6é ­
# æ–°é…ç½® (128M): 10å±¤Ã—768ç¶­Ã—8é ­ - æ¥è¿‘130Mç›®æ¨™
# dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+
bias = False # do we use bias inside LayerNorm and Linear layers?
# adamw optimizer
learning_rate = 3e-4 # max learning rate #longhorn
max_iters = 20000
# max_iters = 600000 # total number of training iterations
weight_decay = 1e-1 #longhorn
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0 
# learning rate decay settings
decay_lr = True # whether to decay the learning rate
warmup_iters = 500
# warmup_iters = 2000 # how many steps to warm up for
lr_decay_iters = 20000
# lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla
# min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
min_lr = 1e-5  # å°é½ŠJAX main
# DDP settings
backend = 'nccl' # 'nccl', 'gloo', etc.
# system
device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
# dtype = 'float32' # å…ˆç”¨float32æ¸¬è©¦ç©©å®šæ€§
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
# å„ªåŒ–ï¼šç¢ºä¿ä½¿ç”¨æœ€ä½³ç²¾åº¦
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True  # å„ªåŒ– cuDNN æ€§èƒ½
compile = False
# compile = True # use PyTorch 2.0 to compile the model to be faster
# -----------------------------------------------------------------------------
# TTT ç‰¹å®šé…ç½® - 130M å„ªåŒ–ç‰ˆæœ¬
ttt_base_lr = 1.0
mini_batch_size = 8   # 130M é…ç½®ï¼šåŒ¹é…æ³¨æ„åŠ›é ­æ•¸ (8é ­)
temperature = 1.0
use_gate = True
share_qk = True       # å•Ÿç”¨ Q/K å…±äº«
ttt_layer_type = "linear"
scan_checkpoint_group_size = 1  # æœ€å°åŒ–æª¢æŸ¥é»ä»¥æå‡é€Ÿåº¦
pre_conv = True  # é‡è¦ï¼šå®˜æ–¹æ‰€æœ‰TTTæ¨¡å‹éƒ½ä½¿ç”¨
conv_kernel = 2  # ğŸ”§ ä¿®æ­£ï¼šèˆ‡ run.sh ä¿æŒä¸€è‡´

# PreCo ç‰¹å®šé…ç½® - èˆ‡ run.sh ä¿æŒä¸€è‡´
longhorn_d_model = 512    # Longhorn æ¨¡å‹ç¶­åº¦
longhorn_n_layer = 12     # Longhorn å±¤æ•¸ (èˆ‡ run.sh ä¸€è‡´)
longhorn_d_state = 8      # Longhorn SSM ç‹€æ…‹ç¶­åº¦ (èˆ‡ run.sh ä¸€è‡´)
longhorn_ssm_expand = 6   # Longhorn SSM æ“´å±•å€æ•¸ (èˆ‡ run.sh ä¸€è‡´)
ttt_hidden_size = 512     # TTT éš±è—å±¤ç¶­åº¦
ttt_num_layers = 1        # TTT å±¤æ•¸ (èˆ‡ run.sh ä¸€è‡´ï¼Œæ¯å±¤ä¸€å€‹ TTTLinear)
ttt_num_heads = 8         # TTT æ³¨æ„åŠ›é ­æ•¸
kalman_hidden_dim = 256   # Kalman ç¶²çµ¡ç¶­åº¦
mlp_ratio = 14            # MLP å€æ•¸ (å¯é€šéå‘½ä»¤è¡Œåƒæ•¸è¦†è“‹)

# æ›´æ–° config_keys
config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]

# ğŸ”§ ç§»é™¤é…ç½®æ–‡ä»¶ä¾è³´ï¼šæ‰€æœ‰é…ç½®éƒ½é€šéå‘½ä»¤è¡Œåƒæ•¸å‚³é
# ä¸å†è¼‰å…¥ä»»ä½•é…ç½®æ–‡ä»¶ï¼Œå®Œå…¨ä¾è³´å‘½ä»¤è¡Œåƒæ•¸å’Œé è¨­å€¼
print(f"âœ… ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸é…ç½®ï¼Œæ¨¡å‹é¡å‹: {model_name}")

# ğŸ”§ ä¿®æ­£ï¼šç¢ºä¿æ¨¡å‹åç¨±æ­£ç¢º
if model_name not in ["ttt", "longhorn", "preco", "preco_nogain"]:
    raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹é¡å‹: {model_name}")

exec(open('configurator.py').read()) # overrides from command line or config file
config = {k: globals()[k] for k in config_keys} # will be useful for logging

# Change to 2% of max iters
# if dataset == "openwebtext":
if dataset == "Slim":
    warmup_iters = min(int(0.15 * max_iters), 3000) # å¢åŠ warmupæ¯”ä¾‹
    #warmup_iters = min(int(0.05 * max_iters), 2000) longhorn

# å–®å¡å®‰å…¨æª¢æŸ¥ï¼šTTT å›ºå®šåˆ‡åˆ†éœ€è¦ block_size å¯æ•´é™¤ mini_batch_size
if model_name == "ttt":
    assert block_size % mini_batch_size == 0, \
        f"block_size({block_size}) å¿…é ˆå¯è¢« mini_batch_size({mini_batch_size}) æ•´é™¤ï¼Œå¦å‰‡å›ºå®šåˆ‡åˆ†æœƒéŒ¯ä½"

# logging
total_tokens = block_size * batch_size * gradient_accumulation_steps * max_iters / 1e9
experiment_name = f"{dataset}_{total_tokens:.1f}_block{block_size}_{model_name}"
wandb_log_interval = 50  # æ¯ 50 æ¬¡è¿­ä»£æ‰è¨˜éŒ„åˆ° wandb

# various inits, derived attributes, I/O setup
ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.
    seed_offset = ddp_rank # each process gets a different seed
    # world_size number of processes will be training simultaneously, so we can scale
    # down the desired gradient accumulation iterations per process proportionally
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size
else:
    # if not ddp, we are running on a single gpu, and one process
    master_process = True
    seed_offset = 0
    ddp_world_size = 1

if master_process:
    os.makedirs(out_dir, exist_ok=True)
torch.manual_seed(master_seed + seed_offset)
torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast
# note: float16 data type will automatically use a GradScaler
ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]
ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)

# poor man's data loader
data_dir = os.path.join('data', dataset)

# ğŸ”§ æ¢å¾©ï¼šä½¿ç”¨ PyTorch åŸç”Ÿçš„ç°¡å–®æ•¸æ“šè¼‰å…¥
print("ğŸš€ ä½¿ç”¨ PyTorch åŸç”Ÿæ•¸æ“šè¼‰å…¥æ–¹å¼")

# è¼‰å…¥é•·é™£åˆ—
train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')
val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')

print(f"âœ… è¼‰å…¥æ•¸æ“š:")
print(f"  Train tokens: {len(train_data):,}")
print(f"  Val tokens: {len(val_data):,}")
print(f"  åºåˆ—é•·åº¦: {block_size}")

# è¼‰å…¥metaè³‡è¨Š
meta_path = os.path.join(data_dir, 'meta.pkl')
if os.path.exists(meta_path):
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    meta_vocab_size = meta.get('vocab_size', 50257)
    print(f"âœ… è¼‰å…¥metaè³‡è¨Š: vocab_size={meta_vocab_size}")
else:
    meta_vocab_size = 50257
    print(f"âš ï¸ æœªæ‰¾åˆ°meta.pklï¼Œä½¿ç”¨é è¨­vocab_size={meta_vocab_size}")

def get_batch(split):
    """PyTorch ç¶“å…¸çš„æ•¸æ“šè¼‰å…¥æ–¹å¼"""
    data = train_data if split == 'train' else val_data
    # éš¨æ©Ÿé¸æ“‡èµ·å§‹ä½ç½®
    ix = torch.randint(len(data) - block_size, (batch_size,))
    # ç›´æ¥åˆ‡ç‰‡ç²å–åºåˆ—
    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
    
    if device_type == 'cuda':
        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)
        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)
    else:
        x, y = x.to(device), y.to(device)
    
    return x, y

# init these up here, can override if init_from='resume' (i.e. from a checkpoint)
iter_num = 0
best_val_loss = 1e9

# attempt to derive vocab_size from the dataset
# æ³¨æ„ï¼šmeta å·²ç¶“åœ¨ä¸Šé¢è®€å–éäº†ï¼Œé€™è£¡ç›´æ¥ä½¿ç”¨
# meta_vocab_size = meta.get('vocab_size', 50257)
# print(f"found vocab_size = {meta_vocab_size} (inside {meta_path})")
# print(f"Using custom tokenizer with vocab_size = {meta_vocab_size}")
# if meta_vocab_size != 32000:  # Llama-2 é è¨­å¤§å°
#     print(f"Note: Using non-standard vocab_size. Ensure model embedding layer matches.")

# model init
if model_name == "preco" or model_name == "preco_nogain":
    # PreCo ç³»åˆ—ä¸ä½¿ç”¨é€šç”¨çš„ model_args
    model_args = {}
else:
    model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,
                    bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line

# # TTT ç‰¹å®šé…ç½®
# ttt_base_lr = 0.1
# mini_batch_size = 8
# temperature = 1.0
# use_gate = True
# ttt_layer_type = "linear"
# scan_checkpoint_group_size = 4

if model_name == "rwkv":
    model_config = RWKVConfig()
    model_config.vocab_size = meta_vocab_size
    model_config.n_layer = n_layer
    model_config.n_head = n_head
    model_config.n_embd = n_embd
    model_config.block_size = block_size
    model_config.bias = bias
    model_config.dropout = dropout
    model = RWKV(model_config)

# elif model_name == "llama":
    # model_config = LLaMAConfig()
    # model_config.vocab_size = meta_vocab_size
    # model_config.n_layer = n_layer
    # model_config.n_head = n_head
    # model_config.n_embd = n_embd
    # model_config.block_size = block_size
    # model_config.bias = bias
    # model_config.dropout = dropout
    # model = LLaMA(model_config)

# elif model_name == "retnet":
    # model_config = RetNetConfig()
    # model_config.vocab_size = meta_vocab_size
    # model_config.decoder_layers = n_layer
    # model_config.decoder_retention_heads = n_head
    # model_config.decoder_embed_dim = n_embd
    # model_config.decoder_value_embed_dim = int(1.8 * n_embd) // n_head * n_head
    # model_config.decoder_ffn_embed_dim = int(1.8 * n_embd) // n_head * n_head
    # model_config.max_target_positions = block_size
    # model = RetNet(model_config)

# elif model_name == "gla":
    # model_config = GLAConfig(
        # d_model=n_embd,
        # n_head=n_head,
        # n_layer=n_layer,
        # context_length=block_size,
        # vocab_size=meta_vocab_size,
    # )
    # model = GLA(model_config)

# elif model_name == "gla_online":
    # model_config = GLAOnlineConfig(
        # d_model=n_embd,
        # n_head=n_head,
        # n_layer=n_layer,
        # context_length=block_size,
        # vocab_size=meta_vocab_size,
        # delta_bias=0.0,
    # )
    # model = GLAOnline(model_config)

elif model_name == "mamba":
    model_config = MambaConfig()
    model_config.vocab_size = meta_vocab_size
    model_config.d_model = n_embd
    model_config.n_layer = 2 * n_layer
    model_config.d_state = 16
    model = Mamba(model_config)

elif model_name == "longhorn":
    model_config = LonghornConfig()
    model_config.vocab_size = meta_vocab_size
    model_config.d_model = n_embd
    model_config.n_layer = n_layer  # 
    model_config.ssm_cfg = {
        'd_state': 16,      # SSM ç‹€æ…‹ç¶­åº¦ 4
        'd_conv': 4,        # å·ç©æ ¸å¤§å° 3
        'expand': 4         # ğŸ¯ èª¿æ•´ï¼šå…§éƒ¨ç¶­åº¦å€æ•¸ (10Ã—512=5120) 10
    }
    model = LonghornLM(model_config)
    
    # Longhorn å…¬å¹³æ¯”è¼ƒé…ç½®æç¤º
    if master_process:
        print("=" * 60)
        print("Longhorn SSM å…¬å¹³æ¯”è¼ƒé…ç½®")
        print("=" * 60)
        print(f"ğŸ¯ å…¬å¹³æ¯”è¼ƒè¨­è¨ˆ:")
        print(f"  1. å±¤æ•¸: {n_layer} å±¤ (èˆ‡å…¶ä»–æ¨¡å‹ä¸€è‡´)")
        print(f"  2. ç¶­åº¦: {n_embd} ç¶­ (èˆ‡å…¶ä»–æ¨¡å‹ä¸€è‡´)")
        print(f"  3. å…§éƒ¨å€æ•¸: expand={model_config.ssm_cfg['expand']} (d_inner={model_config.ssm_cfg['expand']*n_embd})")
        print(f"  4. é æœŸåƒæ•¸é‡: ~130M (expand=11 èª¿æ•´ç‰ˆæœ¬)")
        print(f"  5. SSM ç‹€æ…‹ç¶­åº¦: {model_config.ssm_cfg['d_state']}")
        print("=" * 60)

elif model_name == "ttt":
    # è¨ˆç®—intermediate_size (é€šå¸¸æ˜¯hidden_sizeçš„2.67å€)
    intermediate_size = int(n_embd * 2.67)
    
    model_config = TTTConfig(
        vocab_size=meta_vocab_size,
        hidden_size=n_embd,
        intermediate_size=intermediate_size,  # æ·»åŠ MLPä¸­é–“å±¤ç¶­åº¦
        num_hidden_layers=n_layer,  # ä½¿ç”¨æ–°çš„å±¤æ•¸åƒæ•¸
        num_attention_heads=n_head,
        max_position_embeddings=block_size,
        ttt_base_lr=ttt_base_lr,
        mini_batch_size=mini_batch_size,  # å¹³è¡Œå„ªåŒ–ï¼šæ›´å¤§çš„ mini-batch
        use_gate=use_gate,
        ttt_layer_type=ttt_layer_type,
        scan_checkpoint_group_size=scan_checkpoint_group_size,
        dropout=dropout,
        pre_conv=pre_conv,  # å¹³è¡Œå„ªåŒ–ï¼šé å·ç©
        conv_kernel=conv_kernel,
        # ä¿®æ­£ç‰¹æ®Štoken IDä»¥åŒ¹é…æ‚¨çš„tokenizer
        pad_token_id=0,   # <pad>
        bos_token_id=2,   # <s>
        eos_token_id=3,   # </s>
    )
    model = TTTForCausalLM(model_config)
    
    # TTT å¹³è¡Œè¨“ç·´å„ªåŒ–æç¤º
    if master_process:
        print("=" * 60)
        print("TTT å¹³è¡Œè¨“ç·´å„ªåŒ–é…ç½®")
        print("=" * 60)
        print(f"ğŸš€ å¹³è¡Œå„ªåŒ–ç‰¹é»:")
        print(f"  1. æ›´å¤§çš„ mini-batch: {mini_batch_size} (æ¸›å°‘å¾ªç’°æ¬¡æ•¸)")
        print(f"  2. é å·ç©è™•ç†: {pre_conv}")
        print(f"  3. æœ€å°åŒ–æª¢æŸ¥é»: {scan_checkpoint_group_size}")
        print(f"  4. å„ªåŒ–çš„ TTT å­¸ç¿’ç‡: {ttt_base_lr}")
        print(f"  5. é æœŸé€Ÿåº¦æå‡: 3-5å€")
        print("=" * 60)

elif model_name == "preco":
    model_config = PreCoNewConfig(
        vocab_size=meta_vocab_size,
        d_model=longhorn_d_model,
        n_layer=longhorn_n_layer,
        d_state=longhorn_d_state,
        d_conv=3,  # å›ºå®šç‚º3ï¼Œèˆ‡PreCo.pyä¸€è‡´
        expand=longhorn_ssm_expand,
        ttt_num_heads=ttt_num_heads,
        ttt_num_layers=ttt_num_layers,  # é€™è£¡æ˜¯ 1ï¼Œæ¯å±¤ä¸€å€‹ TTTLinear
        mini_batch_size=mini_batch_size,  # TTT mini batch size
        dropout=dropout,
    )
    model = PreCoNewModel(model_config)
    
    # PreCo æ··åˆæ¨¡å‹è¨“ç·´æç¤º
    if master_process:
        print("=" * 70)
        print("PreCo (Prediction-Correction) æ··åˆæ¨¡å‹é…ç½® - ä¿®æ­£ç‰ˆæ¶æ§‹")
        print("=" * 70)
        print(f"ğŸ”¬ Kalman Filter æ¶æ§‹ç‰¹é»:")
        print(f"  1. Prediction (Longhorn): å¿«é€Ÿå°é–‰è§£é æ¸¬")
        print(f"  2. Correction (TTT): æ¸¬è©¦æ™‚è¨“ç·´æ ¡æ­£")
        print(f"  3. Kalman Gain: Token é‡è¦æ€§å‹•æ…‹æ¬Šé‡")
        print(f"  4. è¯åˆè¨“ç·´: ç«¯åˆ°ç«¯å„ªåŒ–æ‰€æœ‰çµ„ä»¶")
        print(f"")
        print(f"ğŸ¯ ä¿®æ­£å¾Œæ¶æ§‹è¨­è¨ˆ:")
        print(f"  - æ¯å€‹ PreCoBlock: 1å€‹ Longhorn + 1å€‹ TTTLinear + 1å€‹ Kalman Gate")
        print(f"  - ç¸½å±¤æ•¸: {longhorn_n_layer} å±¤ PreCoBlock")
        print(f"  - å¯¦éš› TTT å±¤æ•¸: {longhorn_n_layer} å±¤ (æ¯å±¤ä¸€å€‹ TTTLinear)")
        print(f"  - å…±ç”¨ LM Head è¨­è¨ˆç¯€çœåƒæ•¸")
        print(f"")
        print(f"ğŸ“Š æ¨¡å‹åƒæ•¸ (ä¿®æ­£ç‰ˆ):")
        print(f"  - Longhorn å±¤æ•¸: {longhorn_n_layer}, ç¶­åº¦: {longhorn_d_model}, expand: {longhorn_ssm_expand}")
        print(f"  - TTT å¯¦éš›å±¤æ•¸: {longhorn_n_layer} (æ¯å±¤ {ttt_num_heads} é ­), ç¶­åº¦: {ttt_hidden_size}")
        print(f"  - Kalman ç¶²çµ¡ç¶­åº¦: {kalman_hidden_dim} (å¢å¼·å‹•æ…‹åˆ†é…)")
        print(f"  - å…±ç”¨ LM Head: [{longhorn_d_model}, {meta_vocab_size}] = {longhorn_d_model * meta_vocab_size / 1e6:.1f}M åƒæ•¸")
        print(f"  - åƒæ•¸ç¯€çœ: ç´„ {2 * longhorn_d_model * meta_vocab_size / 1e6:.1f}M (åŸæœ¬éœ€è¦ä¸‰å€‹ç¨ç«‹çš„è¼¸å‡ºå±¤)")
        print(f"")
        print(f"ğŸš€ å„ªåŒ–é‡é»:")
        print(f"  - ä¿®æ­£ TTT å±¤æ•¸ï¼šå¾ {longhorn_n_layer}Ã—{ttt_num_layers} æ”¹ç‚º {longhorn_n_layer}Ã—1")
        print(f"  - åƒæ•¸é‡å„ªåŒ–ï¼šå¾ 180M+ é™è‡³ç´„ 140M")
        print(f"  - å¢å¼· Kalman Gate å‹•æ…‹åˆ†é…")
        print("=" * 70)

elif model_name == "preco_nogain":
    model_config = PreCoNoGainConfig(
        vocab_size=meta_vocab_size,
        d_model=longhorn_d_model,
        n_layer=longhorn_n_layer,
        d_state=longhorn_d_state,
        d_conv=3,  # å›ºå®šç‚º3
        expand=longhorn_ssm_expand,
        ttt_num_heads=ttt_num_heads,
        ttt_num_layers=ttt_num_layers,  # é€™è£¡æ˜¯ 1ï¼Œæ¯å±¤ä¸€å€‹ TTTLinear
        dropout=dropout,
        longhorn_weight=0.7,  # å¯èª¿æ•´
        ttt_weight=0.3,       # å¯èª¿æ•´
    )
    model = PreCoNoGainModel(model_config)
    
    # PreCo NoGain ç°¡åŒ–æ¨¡å‹è¨“ç·´æç¤º
    if master_process:
        print("=" * 70)
        print("PreCo NoGain (ç°¡åŒ–ç‰ˆ PreCo) æ··åˆæ¨¡å‹é…ç½® - 133M åƒæ•¸é‡åŒ¹é…ç‰ˆæœ¬")
        print("=" * 70)
        print(f"ğŸ”¬ ç°¡åŒ–æ¶æ§‹ç‰¹é»:")
        print(f"  1. Prediction (Longhorn): å¿«é€Ÿå°é–‰è§£é æ¸¬")
        print(f"  2. Correction (TTT): æ¸¬è©¦æ™‚è¨“ç·´æ ¡æ­£")
        print(f"  3. ç°¡åŒ–æ¬Šé‡: å¯å­¸ç¿’çš„å…¨å±€æ¬Šé‡èåˆ (ç„¡è¤‡é›œ Kalman Gate)")
        print(f"  4. è¯åˆè¨“ç·´: ç«¯åˆ°ç«¯å„ªåŒ–æ‰€æœ‰çµ„ä»¶")
        print(f"")
        print(f"ğŸ¯ ç°¡åŒ–è¨­è¨ˆå„ªå‹¢:")
        print(f"  - ç§»é™¤è¤‡é›œçš„ TokenButler-style Kalman Gate")
        print(f"  - ä½¿ç”¨ç°¡å–®çš„å¯å­¸ç¿’æ¬Šé‡: Longhorn {model_config.longhorn_weight:.1f} + TTT {model_config.ttt_weight:.1f}")
        print(f"  - æ¸›å°‘è¨“ç·´ä¸ç©©å®šæ€§")
        print(f"  - æ›´å¿«çš„æ”¶æ–‚é€Ÿåº¦")
        print(f"")
        print(f"ğŸ“Š æ¨¡å‹åƒæ•¸ (133M åŒ¹é…ç‰ˆæœ¬):")
        print(f"  - Longhorn å±¤æ•¸: {longhorn_n_layer}, ç¶­åº¦: {longhorn_d_model}, expand: {longhorn_ssm_expand}")
        print(f"  - TTT å±¤æ•¸: {ttt_num_layers}, é ­æ•¸: {ttt_num_heads}, ç¶­åº¦: {ttt_hidden_size}")
        print(f"  - æ¬Šé‡é¡å‹: learnable (å¯åœ¨è¨“ç·´ä¸­èª¿æ•´)")
        print(f"  - å…±ç”¨ LM Head: [{longhorn_d_model}, {meta_vocab_size}] = {longhorn_d_model * meta_vocab_size / 1e6:.1f}M åƒæ•¸")
        print(f"")
        print(f"ğŸš€ å„ªåŒ–é‡é»:")
        print(f"  - ç°¡åŒ–çš„æ¬Šé‡èåˆæ©Ÿåˆ¶")
        print(f"  - æ¸›å°‘è¶…åƒæ•¸èª¿å„ªè¤‡é›œåº¦")
        print(f"  - æ›´ç©©å®šçš„è¨“ç·´éç¨‹")
        print("=" * 70)

else:
    raise Exception(f"Unknown model name {model_name}")

model.to(device)
# åªåœ¨ TTT æ¨¡å‹æ™‚è½‰æ›æ¨¡å‹åƒæ•¸çš„æ•¸æ“šé¡å‹
if model_name == "ttt":
    model = model.to(ptdtype)

# initialize a GradScaler. If enabled=False scaler is a no-op
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))

# optimizer

# start with all of the candidate parameters
param_dict = {pn: p for pn, p in model.named_parameters()}
# filter out those that do not require grad
param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
# create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
# i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]
nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
optim_groups = [
    {'params': decay_params, 'weight_decay': weight_decay},
    {'params': nodecay_params, 'weight_decay': 0.0}
]
num_decay_params = sum(p.numel() for p in decay_params)
num_nodecay_params = sum(p.numel() for p in nodecay_params)
total_params = num_decay_params + num_nodecay_params
print(f"[info] total model params: {total_params/1e6:.1f} M")

# Create AdamW optimizer and use the fused version if it is available
fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
use_fused = fused_available and device_type == 'cuda'
extra_args = dict(fused=True) if use_fused else dict()
optimizer = torch.optim.AdamW(optim_groups,
                              lr=learning_rate,
                              betas=(beta1, beta2),
                              weight_decay=weight_decay,
                              **extra_args)
print(f"using fused AdamW: {use_fused}")
checkpoint = None # free up memory

# ------------------------åˆå§‹åŒ– wandb-----------------------
if master_process:
    # project_name = "preco" if model_name == "preco" else "ttt_slim"
    project_name = "2048"
    wandb_config = {
            "model_name": model_name,
            "dataset": dataset,
            "block_size": block_size,
            "batch_size": batch_size,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "learning_rate": learning_rate,
            "max_iters": max_iters,
            "vocab_size": meta_vocab_size,
            "total_params": total_params,
        }
    
    # æ·»åŠ æ¨¡å‹ç‰¹å®šé…ç½®
    if model_name == "preco":
        wandb_config.update({
            "longhorn_d_model": model_config.d_model,
            "longhorn_n_layer": model_config.n_layer,
            "ttt_num_heads": model_config.ttt_num_heads,
            "ttt_num_layers": model_config.ttt_num_layers,
            "d_state": model_config.d_state,
            "expand": model_config.expand,
            "dropout": model_config.dropout,
            "training_objective": "corrected_state_cross_entropy_only",
            "shared_lm_head": True,
            "shared_components": "longhorn_C + ttt_wo + preco_q",
            "parameter_sharing": "ä¸‰åˆä¸€å…±ç”¨è¨­è¨ˆ",
            "shared_lm_head_params": f"{model_config.d_model * meta_vocab_size / 1e6:.1f}M",
            "parameter_savings": f"{2 * model_config.d_model * meta_vocab_size / 1e6:.1f}M",
        })
    elif model_name == "preco_nogain":
        wandb_config.update({
            "longhorn_d_model": model_config.d_model,
            "longhorn_n_layer": model_config.n_layer,
            "ttt_num_heads": model_config.ttt_num_heads,
            "ttt_num_layers": model_config.ttt_num_layers,
            "d_state": model_config.d_state,
            "expand": model_config.expand,
            "dropout": model_config.dropout,
            "longhorn_weight": model_config.longhorn_weight,
            "ttt_weight": model_config.ttt_weight,
            "training_objective": "simplified_weight_fusion",
            "shared_lm_head": True,
            "architecture_type": "preco_nogain_simplified",
            "shared_lm_head_params": f"{model_config.d_model * meta_vocab_size / 1e6:.1f}M",
            "parameter_savings": f"{2 * model_config.d_model * meta_vocab_size / 1e6:.1f}M",
        })
    else:
        wandb_config.update({
            "n_layer": n_layer,
            "n_head": n_head,
            "n_embd": n_embd,
        })
    
    wandb.init(
        project=project_name,
        name=experiment_name,
        config=wandb_config
    )

tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size
print(f"tokens per iteration will be: {tokens_per_iter:,}")

# compile the model
if compile:
    print("compiling the model... (takes a ~minute)")
    unoptimized_model = model
    model = torch.compile(model) # requires PyTorch 2.0

# wrap model into DDP container
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])

# æ·»åŠ  PPL è¨ˆç®—å‡½æ•¸
def calculate_ppl(loss):
    if loss > 700:  # exp(700) æ¥è¿‘ float ä¸Šé™
        return float('inf')
    try:
        return math.exp(loss)
    except OverflowError:
        return float('inf')

# ä¿®æ”¹ estimate_loss å‡½æ•¸
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        # æ”¶é›†æœ‰æ•ˆ batch çš„ lossï¼ˆé¡¯ç¤ºç”¨ï¼‰èˆ‡ PPL ç”¨çš„ CE/NLL
        losses_list: list = []
        ppl_losses_list: list = []
        # è¿½åŠ ï¼šåœ¨è©•ä¼°æ™‚è¨ˆç®—ç°¡å–®è¨ºæ–·æŒ‡æ¨™
        accs: list = []
        max_probs: list = []
        
        # PreCo ç³»åˆ—ç‰¹å®šçµ±è¨ˆ
        if model_name == "preco" or model_name == "preco_nogain":
            ce_losses: list = []
            longhorn_losses: list = []
            ttt_losses: list = []
            kalman_means: list = []
            kalman_stds: list = []
        bad_batches = 0
            
        for k in range(eval_iters):
            x, y = get_batch(split)
            # è©•ä¼°é¡¯å¼é—œé–‰ AMPï¼Œè‡ªå‹•ä»¥ FP32 è¨ˆç®—æ›´ç©©å®š
            from torch.cuda.amp import autocast
            with autocast(enabled=False):
                if model_name == "ttt":
                    outputs = model(
                        input_ids=x, 
                        labels=y
                    )
                    # æª¢æŸ¥è¼¸å‡ºæ ¼å¼
                    if isinstance(outputs, tuple):
                        logits, loss = outputs[0], outputs[1]
                    else:
                        logits, loss = outputs.logits, outputs.loss
                    # è¿½åŠ ï¼šè¨ˆç®—shiftå¾Œçš„top-1 accuracyèˆ‡å¹³å‡æœ€å¤§æ©Ÿç‡
                    try:
                        shift_logits = logits[..., :-1, :].float()
                        shift_labels = y[..., 1:].long()
                        preds = shift_logits.argmax(dim=-1)
                        acc_val = (preds == shift_labels).float().mean().item()
                        # å¹³å‡æœ€å¤§æ©Ÿç‡
                        max_prob_val = torch.softmax(shift_logits, dim=-1).max(dim=-1)[0].mean().item()
                        accs.append(acc_val)
                        max_probs.append(max_prob_val)
                    except Exception:
                        pass
                    # è¨˜éŒ„ loss èˆ‡ PPL ç”¨çš„ NLLï¼ˆCEï¼‰
                    loss_val = float(loss.item()) if hasattr(loss, 'item') else float(loss)
                    if math.isfinite(loss_val):
                        losses_list.append(loss_val)
                        ppl_losses_list.append(loss_val)
                    else:
                        bad_batches += 1
                elif model_name == "preco" or model_name == "preco_nogain":
                    if model_name == "preco":
                        # âœ¨ PreCo æ–°ç‰ˆï¼šä½¿ç”¨ TTT åŸç”Ÿè‡ªé©æ‡‰å­¸ç¿’ç‡
                        logits, loss_dict = model(
                            x,
                            y,
                            compute_branch_loss=False,
                        )
                    else:  # preco_nogain
                        # PreCo NoGain ä»ä½¿ç”¨å¤–éƒ¨èª¿åº¦
                        ttt_lr_mult = get_ttt_lr_mult(iter_num)
                        logits, loss_dict = model(
                            x,
                            y,
                            ttt_lr_mult=ttt_lr_mult,
                            compute_branch_loss=False,
                        )
                    
                    # ç¢ºä¿æ‰€æœ‰ loss_dict å€¼éƒ½è½‰ç‚º floatï¼ˆä¸è¦†è“‹éæœ‰é™å€¼ï¼‰
                    for key in list(loss_dict.keys()):
                        if hasattr(loss_dict[key], 'item'):
                            loss_dict[key] = loss_dict[key].item()
                        else:
                            loss_dict[key] = float(loss_dict[key])
                    total_loss_val = float(loss_dict.get('total_loss', float('nan')))
                    ce_loss_val = float(loss_dict.get('ce_loss', float('nan')))
                    # åƒ…åœ¨å…©è€…æœ‰é™æ™‚ç´å…¥ä¸»çµ±è¨ˆï¼›PPL ä½¿ç”¨ CE loss
                    if math.isfinite(total_loss_val) and math.isfinite(ce_loss_val):
                        losses_list.append(total_loss_val)
                        ppl_losses_list.append(ce_loss_val)
                        # è¨˜éŒ„ç°¡åŒ–ç‰ˆçš„æå¤±çµ±è¨ˆ
                        ce_losses.append(ce_loss_val)
                        longhorn_losses.append(float(loss_dict.get('longhorn_loss', 0.0)))
                        ttt_losses.append(float(loss_dict.get('ttt_loss', 0.0)))
                        # æ ¹æ“šæ¨¡å‹é¡å‹ä½¿ç”¨ä¸åŒçš„çµ±è¨ˆå­—æ®µ
                        if model_name == "preco":
                            kalman_means.append(float(loss_dict.get('kalman_mean', 0.0)))
                            kalman_stds.append(float(loss_dict.get('kalman_std', 0.0)))
                        else:
                            kalman_means.append(float(loss_dict.get('weight_mean', 0.0)))
                            kalman_stds.append(float(loss_dict.get('weight_std', 0.0)))
                    else:
                        bad_batches += 1
                else:
                    logits, loss = model(x, y)
                    # å…¶å®ƒæ¨¡å‹ï¼šåªè¦ loss æœ‰é™å°±è¨ˆå…¥ï¼ŒPPL ä½¿ç”¨åŒä¸€ loss
                    loss_val = float(loss.item()) if hasattr(loss, 'item') else float(loss)
                    if math.isfinite(loss_val):
                        losses_list.append(loss_val)
                        ppl_losses_list.append(loss_val)
                    else:
                        bad_batches += 1
            
        mean_loss = (sum(losses_list) / len(losses_list)) if len(losses_list) > 0 else float('inf')
        mean_ppl_loss = (sum(ppl_losses_list) / len(ppl_losses_list)) if len(ppl_losses_list) > 0 else float('inf')
        result = {
            'loss': float(mean_loss),
            'ppl': calculate_ppl(float(mean_ppl_loss))
        }
        # å›å‚³è¨ºæ–·æŒ‡æ¨™
        if model_name == "ttt":
            if len(accs) > 0:
                result['acc'] = float(sum(accs) / len(accs))
            if len(max_probs) > 0:
                result['max_prob'] = float(sum(max_probs) / len(max_probs))
        
        # æ·»åŠ  PreCo ç³»åˆ—çµ±è¨ˆ
        if model_name == "preco" or model_name == "preco_nogain":
            result.update({
                'ce_loss': float(sum(ce_losses)/len(ce_losses)) if len(ce_losses)>0 else float('inf'),
                'longhorn_loss': float(sum(longhorn_losses)/len(longhorn_losses)) if len(longhorn_losses)>0 else float('inf'),
                'ttt_loss': float(sum(ttt_losses)/len(ttt_losses)) if len(ttt_losses)>0 else float('inf'),
                'kalman_mean': float(sum(kalman_means)/len(kalman_means)) if len(kalman_means)>0 else 0.0,
                'kalman_std': float(sum(kalman_stds)/len(kalman_stds)) if len(kalman_stds)>0 else 0.0,
                'dropped_batches': bad_batches,
            })
            
        out[split] = result
    model.train()
    return out

# learning rate decay scheduler (cosine with warmup)
def get_lr(it):
    # 1) ç·šæ€§ warmup
    if it < warmup_iters:
        return learning_rate * it / warmup_iters
    # 2) é”åˆ°æˆ–è¶…éè¡°æ¸›ç¸½æ­¥æ•¸ï¼šå›ºå®šç‚º min_lr
    if it >= lr_decay_iters:
        return min_lr
    # 3) ä¸­é–“å€æ®µï¼šæ¨™æº–é¤˜å¼¦è¡°æ¸›
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    decay_ratio = max(0.0, min(1.0, decay_ratio))
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate - min_lr)

# æ·»åŠ  TTT å­¸ç¿’ç‡èª¿åº¦ - ä¿®æ­£ç‰ˆ
def get_ttt_lr_mult(it):
    """TTT å­¸ç¿’ç‡èª¿åº¦ - ä¿®æ­£ç‰ˆï¼Œé‚è¼¯æ¸…æ™°"""
    if model_name == "ttt" or model_name == "preco" or model_name == "preco_nogain":
        base_mult = 1.0
        
        # ğŸ”§ æ›´å¹³æ»‘çš„ warmup ç­–ç•¥
        warmup_steps = 500  # å¢åŠ åˆ° 500 iterï¼Œæ›´å¹³æ»‘
        
        if it < warmup_steps:
            # ä½¿ç”¨é¤˜å¼¦ warmupï¼Œæ›´å¹³æ»‘
            progress = it / warmup_steps
            # å¾ 0.1 é–‹å§‹ï¼Œä½¿ç”¨é¤˜å¼¦å‡½æ•¸å¹³æ»‘åˆ° 1.0
            return 0.1 + 0.9 * (1.0 - math.cos(math.pi * progress)) / 2.0
        
        # 500-2000 iterï¼šæ¨™æº–å­¸ç¿’ç‡
        elif it < 2000:
            return base_mult
        
        # ğŸ”§ æ›´å¹³æ»‘çš„è¡°æ¸›ç­–ç•¥
        elif it >= 3000:  # æå‰é–‹å§‹è¡°æ¸›
            decay_start = 3000
            decay_ratio = (it - decay_start) / (lr_decay_iters - decay_start)
            decay_ratio = min(decay_ratio, 1.0)
            
            # ä½¿ç”¨é¤˜å¼¦è¡°æ¸›ï¼Œæ›´å¹³æ»‘
            min_ttt_mult = 0.3  # é™ä½æœ€å°å€¼
            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
            return min_ttt_mult + coeff * (base_mult - min_ttt_mult)
        
        # 2000-3000 iterï¼šä¿æŒæ¨™æº–æ¿€æ´»
        else:  # it >= 2000 and it < 3000
            return base_mult
        
    return 1.0

# ğŸ”§ æ–°å¢ï¼šTrain/Val gap ç›£æ§å’Œæ­£å‰‡åŒ–æ©Ÿåˆ¶
def calculate_train_val_gap(train_loss, val_loss):
    """è¨ˆç®— Train/Val gap ä¸¦è¿”å›éæ“¬åˆæŒ‡æ¨™"""
    gap = val_loss - train_loss
    gap_ratio = gap / train_loss if train_loss > 0 else 0
    return gap, gap_ratio

def get_adaptive_regularization(iter_num, train_loss, val_loss, base_weight_decay=1e-1):
    """æ ¹æ“š Train/Val gap è‡ªé©æ‡‰èª¿æ•´æ­£å‰‡åŒ–å¼·åº¦"""
    gap, gap_ratio = calculate_train_val_gap(train_loss, val_loss)
    
    # åŸºç¤æ­£å‰‡åŒ–æ¬Šé‡
    adaptive_weight_decay = base_weight_decay
    
    # ğŸ”§ 2000+ iter å¾Œå•Ÿç”¨è‡ªé©æ‡‰æ­£å‰‡åŒ–
    if iter_num >= 2000:
        # å¦‚æœ gap_ratio > 0.05 (5%)ï¼Œå¢åŠ æ­£å‰‡åŒ–
        if gap_ratio > 0.05:
            adaptive_weight_decay = base_weight_decay * 2.0
        # å¦‚æœ gap_ratio > 0.1 (10%)ï¼Œé€²ä¸€æ­¥å¢åŠ æ­£å‰‡åŒ–
        if gap_ratio > 0.1:
            adaptive_weight_decay = base_weight_decay * 3.0
        # å¦‚æœ gap_ratio > 0.15 (15%)ï¼Œæœ€å¤§æ­£å‰‡åŒ–
        if gap_ratio > 0.15:
            adaptive_weight_decay = base_weight_decay * 4.0
    
    return adaptive_weight_decay, gap, gap_ratio

# training loop
train_stats = {
    "experiment_name": experiment_name,
    "global_args": config,
    "model/config": vars(model_config),
    "model/params": total_params,
    "total_tokens": total_tokens,
    "iter": [],
    "train/loss": [],
    "val/loss": [],
    "train/ppl": [],
    "val/ppl": [],
    "lr": [],
}

x, y = get_batch('train') # fetch the very first batch
t0 = time.time()
local_iter_num = 0 # number of iterations in the lifetime of this process
raw_model = model.module if ddp else model # unwrap DDP container if needed
running_mfu = -1.0
while True:

    # determine and set the learning rate for this iteration
    lr = get_lr(iter_num) if decay_lr else learning_rate
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

    # evaluate the loss on train/val sets and write checkpoints
    if iter_num % eval_interval == 0 and master_process and do_eval:
        losses = estimate_loss()
        train_loss = losses['train']['loss']
        val_loss = losses['val']['loss']
        
        # ğŸ”§ æ–°å¢ï¼šè¨ˆç®— Train/Val gap å’Œè‡ªé©æ‡‰æ­£å‰‡åŒ–
        adaptive_weight_decay, gap, gap_ratio = get_adaptive_regularization(
            iter_num, train_loss, val_loss, weight_decay
        )
        
        print(f"step {iter_num}: train loss {train_loss:.4f}, val loss {val_loss:.4f}")
        print(f"train ppl {losses['train']['ppl']:.4f}, val ppl {losses['val']['ppl']:.4f}")
        
        # ğŸ”§ æ–°å¢ï¼šTrain/Val gap ç›£æ§è¼¸å‡º
        print(f"ğŸ“ˆ Train/Val Gap: {gap:.4f} ({gap_ratio*100:.2f}%)")
        if iter_num >= 2000:
            print(f"ğŸ”§ Adaptive Weight Decay: {adaptive_weight_decay:.6f} (base: {weight_decay:.6f})")
            if gap_ratio > 0.05:
                print(f"âš ï¸  éæ“¬åˆè­¦å‘Š: Gap ratio {gap_ratio*100:.2f}% > 5%, å·²å¢åŠ æ­£å‰‡åŒ–")
        
        # PreCo ç³»åˆ—æ¨¡å‹çš„è©³ç´°è¼¸å‡º
        if model_name == "preco":
            print(f"  ğŸ“Š PreCo è©³ç´°çµ±è¨ˆ:")
            print(f"    CE Loss: {losses['train']['ce_loss']:.4f} / {losses['val']['ce_loss']:.4f}")
            print(f"    ç›£æ§æŒ‡æ¨™:")
            print(f"      Longhorn Loss: {losses['train']['longhorn_loss']:.4f} / {losses['val']['longhorn_loss']:.4f}")
            print(f"      TTT Loss: {losses['train']['ttt_loss']:.4f} / {losses['val']['ttt_loss']:.4f}")
            print(f"    Kalman Gain åˆ†æ:")
            print(f"      å¹³å‡å€¼: {losses['train']['kalman_mean']:.4f} / {losses['val']['kalman_mean']:.4f}")
            print(f"      æ¨™æº–å·®: {losses['train']['kalman_std']:.4f} / {losses['val']['kalman_std']:.4f}")
        elif model_name == "preco_nogain":
            print(f"  ğŸ“Š PreCo NoGain è©³ç´°çµ±è¨ˆ:")
            print(f"    CE Loss: {losses['train']['ce_loss']:.4f} / {losses['val']['ce_loss']:.4f}")
            print(f"    ç›£æ§æŒ‡æ¨™:")
            print(f"      Longhorn Loss: {losses['train']['longhorn_loss']:.4f} / {losses['val']['longhorn_loss']:.4f}")
            print(f"      TTT Loss: {losses['train']['ttt_loss']:.4f} / {losses['val']['ttt_loss']:.4f}")
            print(f"    ç°¡åŒ–æ¬Šé‡åˆ†æ:")
            print(f"      å¹³å‡å€¼: {losses['train']['kalman_mean']:.4f} / {losses['val']['kalman_mean']:.4f}")
            print(f"      æ¨™æº–å·®: {losses['train']['kalman_std']:.4f} / {losses['val']['kalman_std']:.4f}")
        
        # ğŸ”§ æ–°å¢ï¼šå‹•æ…‹èª¿æ•´å„ªåŒ–å™¨çš„æ¬Šé‡è¡°æ¸›
        if iter_num >= 2000 and adaptive_weight_decay != weight_decay:
            for param_group in optimizer.param_groups:
                if 'weight_decay' in param_group:
                    param_group['weight_decay'] = adaptive_weight_decay
        
        train_stats['iter'].append(iter_num)
        train_stats['train/loss'].append(losses['train']['loss'])
        train_stats['val/loss'].append(losses['val']['loss'])
        train_stats['train/ppl'].append(losses['train']['ppl'])
        train_stats['val/ppl'].append(losses['val']['ppl'])
        train_stats['lr'].append(lr)
        
        # è¨˜éŒ„åˆ° wandb - ç°¡åŒ–ç‰ˆæœ¬
        if master_process:
            log_data = {
                "train/loss": losses['train']['loss'],
                "val/loss": losses['val']['loss'],
                "train/ppl": losses['train']['ppl'],
                "val/ppl": losses['val']['ppl'],
                "learning_rate": lr,
                "iter": iter_num,
            }
            
            # PreCo ç³»åˆ—æ¨¡å‹çš„è¨˜éŒ„
            if model_name == "preco":
                def _f(x):
                    try:
                        return float(x)
                    except Exception:
                        return x
                log_data.update({
                    "preco/train_loss": _f(losses['train']['loss']),
                    "preco/val_loss": _f(losses['val']['loss']),
                    "preco/train_ce_loss": _f(losses['train']['ce_loss']),
                    "preco/val_ce_loss": _f(losses['val']['ce_loss']),
                    "preco/train_longhorn_loss": _f(losses['train']['longhorn_loss']),
                    "preco/val_longhorn_loss": _f(losses['val']['longhorn_loss']),
                    "preco/train_ttt_loss": _f(losses['train']['ttt_loss']),
                    "preco/val_ttt_loss": _f(losses['val']['ttt_loss']),
                    "preco/train_kalman_mean": _f(losses['train']['kalman_mean']),
                    "preco/val_kalman_mean": _f(losses['val']['kalman_mean']),
                    "preco/train_kalman_std": _f(losses['train']['kalman_std']),
                    "preco/val_kalman_std": _f(losses['val']['kalman_std']),
                    "preco/adaptive_ttt_lr": "native",  # âœ¨ æ¨™è¨˜ä½¿ç”¨ TTT åŸç”Ÿè‡ªé©æ‡‰å­¸ç¿’ç‡
                    "preco/branch_use": int(use_branch_losses),
                    "preco/branch_longhorn_w": float(branch_longhorn_weight),
                    "preco/branch_ttt_w": float(branch_ttt_weight),
                })
            elif model_name == "preco_nogain":
                log_data.update({
                    "preco_nogain/train_loss": losses['train']['loss'],
                    "preco_nogain/val_loss": losses['val']['loss'],
                    "preco_nogain/train_ce_loss": losses['train']['ce_loss'],
                    "preco_nogain/val_ce_loss": losses['val']['ce_loss'],
                    "preco_nogain/train_longhorn_loss": losses['train']['longhorn_loss'],
                    "preco_nogain/val_longhorn_loss": losses['val']['longhorn_loss'],
                    "preco_nogain/train_ttt_loss": losses['train']['ttt_loss'],
                    "preco_nogain/val_ttt_loss": losses['val']['ttt_loss'],
                    "preco_nogain/train_weight_mean": losses['train']['kalman_mean'],
                    "preco_nogain/val_weight_mean": losses['val']['kalman_mean'],
                    "preco_nogain/train_weight_std": losses['train']['kalman_std'],
                    "preco_nogain/val_weight_std": losses['val']['kalman_std'],
                    "preco_nogain/ttt_lr_mult": get_ttt_lr_mult(iter_num),
                })
            wandb.log(log_data)
        
        # å·²ç§»é™¤ PPL ç¹ªåœ–/ä¿å­˜
        
        torch.save(train_stats,
                   os.path.join(out_dir, f'{experiment_name}.stats'))

        if losses['val']['loss'] < best_val_loss or always_save_checkpoint:
            best_val_loss = losses['val']['loss']
            if iter_num > 0:
                checkpoint = {
                    'model': raw_model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'global_args': config,
                    'model_config': vars(model_config),
                    'iter_num': iter_num,
                    'best_val_loss': best_val_loss,
                    'config': config,
                }
                # é™„åŠ æ¨¡å‹ç‰¹å®šçš„è¼”åŠ©ä¸­ç¹¼è³‡æ–™ï¼ˆä¸é‡è¤‡ä¿å­˜åƒæ•¸ï¼‰
                if model_name == "preco":
                    # ä¿å­˜ PreCo ç‰¹å®šå…ƒä¿¡æ¯ - ä¸é‡è¤‡ä¿å­˜åƒæ•¸ï¼ˆå·²åœ¨ model.state_dict() ä¸­ï¼‰
                    preco_meta = {
                        # æ¨¡å‹æ¶æ§‹ä¿¡æ¯
                        'shared_components': ['longhorn_backbone', 'ttt_backbone', 'q_network'],
                        'parameter_sharing_info': 'Longhorn backbone + TTT backbone + q_network å°ˆç”¨è¼¸å‡º',
                        'architecture_type': 'kalman_filter_with_q_network',
                        # çµ„ä»¶æ˜ å°„ä¿¡æ¯ï¼ˆç”¨æ–¼è¼‰å…¥æ™‚é©—è­‰ï¼‰
                        'component_mapping': {
                            'longhorn_output': 'q_network',
                            'ttt_output': 'q_network',
                            'final_output': 'q_network',
                        },
                        # PreCoNewModel æ²’æœ‰ q_network å’Œ kalman_gain å±¬æ€§ï¼Œç§»é™¤ç›¸é—œçµ±è¨ˆ
                    }
                    checkpoint['preco_meta'] = preco_meta
                elif model_name == "preco_nogain":
                    preco_nogain_meta = {
                        'shared_components': ['longhorn_backbone', 'ttt_backbone', 'q_network'],
                        'parameter_sharing_info': 'Longhorn backbone + TTT backbone + q_network å°ˆç”¨è¼¸å‡º',
                        'architecture_type': 'preco_nogain_simplified',
                        'weight_type': 'learnable',
                        'longhorn_weight': model_config.longhorn_weight,
                        'ttt_weight': model_config.ttt_weight,
                        'component_mapping': {
                            'longhorn_output': 'q_network',
                            'ttt_output': 'q_network',
                            'final_output': 'q_network',
                        },
                    }
                    checkpoint['preco_nogain_meta'] = preco_nogain_meta
                print(f"saving checkpoint to {out_dir}")
                torch.save(checkpoint,
                          os.path.join(out_dir, f'{experiment_name}_best.pt'))

    # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨å¯é…ç½®çš„æª¢æŸ¥é»é–“éš” - ğŸš€ å„ªåŒ–ï¼šæ¸›å°‘ä¿å­˜é »ç‡
    checkpoint_interval = globals().get('checkpoint_interval', 1000)  # å¾ 500 æ”¹ç‚º 1000
    if iter_num % checkpoint_interval == 0 and iter_num > 0:
        checkpoint = {
            'model': raw_model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'global_args': config,
            'model_config': vars(model_config),
            'iter_num': iter_num,
            'best_val_loss': best_val_loss,
            'config': config,
        }
        if model_name == "preco":
            # ä¿å­˜ PreCo ç‰¹å®šå…ƒä¿¡æ¯ - ä¸é‡è¤‡ä¿å­˜åƒæ•¸ï¼ˆå·²åœ¨ model.state_dict() ä¸­ï¼‰
            preco_meta = {
                # æ¨¡å‹æ¶æ§‹ä¿¡æ¯
                'shared_components': ['longhorn_backbone', 'ttt_backbone', 'q_network'],
                'parameter_sharing_info': 'Longhorn backbone + TTT backbone + q_network å°ˆç”¨è¼¸å‡º',
                'architecture_type': 'kalman_filter_with_q_network',
                # çµ„ä»¶æ˜ å°„ä¿¡æ¯ï¼ˆç”¨æ–¼è¼‰å…¥æ™‚é©—è­‰ï¼‰
                'component_mapping': {
                    'longhorn_output': 'q_network',
                    'ttt_output': 'q_network',
                    'final_output': 'q_network',
                },
            }
            checkpoint['preco_meta'] = preco_meta
        elif model_name == "preco_nogain":
            # ä¿å­˜ PreCo NoGain ç‰¹å®šå…ƒä¿¡æ¯
            preco_nogain_meta = {
                # æ¨¡å‹æ¶æ§‹ä¿¡æ¯
                'shared_components': ['longhorn_backbone', 'ttt_backbone', 'q_network'],
                'parameter_sharing_info': 'Longhorn backbone + TTT backbone + q_network å°ˆç”¨è¼¸å‡º',
                'architecture_type': 'preco_nogain_simplified',
                'weight_type': 'learnable',
                'longhorn_weight': model_config.longhorn_weight,
                'ttt_weight': model_config.ttt_weight,
                # çµ„ä»¶æ˜ å°„ä¿¡æ¯ï¼ˆç”¨æ–¼è¼‰å…¥æ™‚é©—è­‰ï¼‰
                'component_mapping': {
                    'longhorn_output': 'q_network',
                    'ttt_output': 'q_network',
                    'final_output': 'q_network',
                },
            }
            checkpoint['preco_nogain_meta'] = preco_nogain_meta
        print(f"saving periodic checkpoint to {out_dir}")
        torch.save(checkpoint,
                  os.path.join(out_dir, f'{experiment_name}_iter{iter_num}.pt'))

    if iter_num == 0 and eval_only:
        break

    # forward backward update, with optional gradient accumulation to simulate larger batch size
    # and using the GradScaler if data type is float16
    for micro_step in range(gradient_accumulation_steps):
        if ddp:
            # in DDP training we only need to sync gradients at the last micro step.
            # the official way to do this is with model.no_sync() context manager, but
            # I really dislike that this bloats the code and forces us to repeat code
            # looking at the source of that context manager, it just toggles this variable
            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)
        with ctx:
            if model_name == "ttt":
                # TTT æ¨¡å‹ï¼šä½¿ç”¨æ¨™æº– Cross-Entropy Loss
                outputs = model(
                    input_ids=x, 
                    labels=y
                )
                # æª¢æŸ¥è¼¸å‡ºæ ¼å¼
                if isinstance(outputs, tuple):
                    logits, loss = outputs[0], outputs[1]
                else:
                    logits, loss = outputs.logits, outputs.loss
                # åªä½¿ç”¨ä¸»è¦çš„ Cross-Entropy loss
                loss = loss / gradient_accumulation_steps
            elif model_name == "preco":
                # PreCo æ··åˆæ¨¡å‹å‰å‘å‚³æ’­
                # âœ¨ æ–°ç‰ˆï¼šä¸å†å‚³é ttt_lr_multï¼Œä½¿ç”¨ TTT åŸç”Ÿè‡ªé©æ‡‰å­¸ç¿’ç‡
                logits, loss_dict = model(
                    x,
                    y,
                    compute_branch_loss=False,
                )
                loss = loss_dict['total_loss'] / gradient_accumulation_steps
            elif model_name == "preco_nogain":
                # PreCo NoGain ç°¡åŒ–æ¨¡å‹å‰å‘å‚³æ’­
                ttt_lr_mult = get_ttt_lr_mult(iter_num)
                logits, loss_dict = model(x, y, ttt_lr_mult=ttt_lr_mult)
                loss = loss_dict['total_loss'] / gradient_accumulation_steps
            else:
                logits, loss = model(x, y)
                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation

        # immediately async prefetch next batch while model is doing the forward pass on the GPU
        x, y = get_batch('train')
        # backward pass, with gradient scaling if training in fp16
        scaler.scale(loss).backward()

    # clip the gradient
    if grad_clip != 0.0:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)

    # step the optimizer and scaler if training in fp16
    scaler.step(optimizer)
    scaler.update()
    # flush the gradients as soon as we can, no need for this memory anymore
    optimizer.zero_grad(set_to_none=True)

    # timing and logging - ğŸš€ å„ªåŒ–ï¼šæ¸›å°‘åŒæ­¥é»å’Œæ—¥èªŒé–‹éŠ·
    t1 = time.time()
    dt = t1 - t0
    t0 = t1
    
    # ğŸš€ å„ªåŒ–ï¼šæ¸›å°‘æ—¥èªŒé »ç‡ï¼Œåªåœ¨é—œéµæ™‚åˆ»åŒæ­¥
    if iter_num % (log_interval * 2) == 0 and master_process:  # å¾æ¯ 50 æ¬¡æ”¹ç‚ºæ¯ 100 æ¬¡
        # get loss as float. note: this is a CPU-GPU sync point
        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)
        lossf = loss.item() * gradient_accumulation_steps
        print(f"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms", flush=True)
        
        # ğŸš€ å„ªåŒ–ï¼šæ¸›å°‘ wandb åŒæ­¥é »ç‡
        if iter_num % (wandb_log_interval * 2) == 0 and master_process:  # å¾æ¯ 50 æ¬¡æ”¹ç‚ºæ¯ 100 æ¬¡
            log_data = {
                "train/iter_loss": lossf,
                "train/iter_time": dt*1000,
                "iter": iter_num
            }
            # PreCo ç³»åˆ—æ¨¡å‹çš„å³æ™‚çµ±è¨ˆè¨˜éŒ„
            if model_name == "preco":
                log_data["preco/adaptive_ttt_lr"] = "native"  # âœ¨ æ¨™è¨˜ä½¿ç”¨åŸç”Ÿè‡ªé©æ‡‰å­¸ç¿’ç‡
                # å¦‚æœæœ‰ loss_dictï¼Œè¨˜éŒ„è©³ç´°æå¤±ï¼ˆè½‰ç‚ºç´” floatï¼Œé¿å… CUDA tensor é€ æˆ 0/NaNï¼‰
                if 'loss_dict' in locals():
                    def _to_float(v):
                        try:
                            return float(v.detach().float().item()) if hasattr(v, 'detach') else (float(v.item()) if hasattr(v, 'item') else float(v))
                        except Exception:
                            return float(v)
                    log_data.update({
                        "preco/iter_loss": float(lossf),
                        "preco/iter_ce_loss": _to_float(loss_dict['ce_loss']),
                        "preco/iter_longhorn_loss": _to_float(loss_dict['longhorn_loss']),
                        "preco/iter_ttt_loss": _to_float(loss_dict['ttt_loss']),
                        "preco/iter_kalman_mean": _to_float(loss_dict['kalman_mean']),
                        "preco/iter_kalman_std": _to_float(loss_dict['kalman_std']),
                    })
                print(f"  âœ¨ TTT è‡ªé©æ‡‰å­¸ç¿’ç‡: åŸç”Ÿæ©Ÿåˆ¶")
            elif model_name == "preco_nogain":
                log_data["preco_nogain/ttt_lr_mult"] = get_ttt_lr_mult(iter_num)
                # å¦‚æœæœ‰ loss_dictï¼Œè¨˜éŒ„è©³ç´°æå¤±
                if 'loss_dict' in locals():
                    log_data.update({
                        "preco_nogain/iter_loss": lossf,
                        "preco_nogain/iter_ce_loss": loss_dict['ce_loss'],
                        "preco_nogain/iter_longhorn_loss": loss_dict['longhorn_loss'],
                        "preco_nogain/iter_ttt_loss": loss_dict['ttt_loss'],
                        "preco_nogain/iter_weight_mean": loss_dict.get('longhorn_weight_mean', 0.0),
                        "preco_nogain/iter_weight_std": loss_dict.get('weight_std', 0.0),
                        "preco_nogain/iter_weight_entropy": loss_dict.get('weight_entropy', 0.0),
                    })
                print(f"  TTT lr mult: {get_ttt_lr_mult(iter_num):.4f}")
            wandb.log(log_data)



    iter_num += 1
    local_iter_num += 1

    # termination conditions
    if iter_num > max_iters:
        break

if ddp:
    destroy_process_group()

# -----------------çµæŸæ™‚é—œé–‰ wandb-----------------
if master_process:
    wandb.finish()
