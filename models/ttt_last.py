import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import PretrainedConfig
from transformers.modeling_utils import PreTrainedModel
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from typing import List, Optional, Tuple, Union, Dict, Any

from collections import defaultdict
from dataclasses import dataclass
from torch import nn
from torch.nn import CrossEntropyLoss
from torch.utils._pytree import tree_map

from transformers.activations import ACT2FN
from transformers.utils import ModelOutput, logging
from transformers.utils.import_utils import is_causal_conv1d_available

if is_causal_conv1d_available():
    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
else:
    causal_conv1d_update, causal_conv1d_fn = None, None

logger = logging.get_logger(__name__)

TTT_STANDARD_CONFIGS = {
    "125m": {
        "hidden_size": 768,
        "intermediate_size": 2048,
        "num_hidden_layers": 12,
        "num_attention_heads": 12,
    },
    "350m": {
        "hidden_size": 1024,
        "intermediate_size": 2736,
        "num_hidden_layers": 24,
        "num_attention_heads": 16,
    },
    "760m": {
        "hidden_size": 1536,
        "intermediate_size": 4096,
        "num_hidden_layers": 24,
        "num_attention_heads": 16,
    },
    "1b": {
        "hidden_size": 2048,
        "intermediate_size": 5504,
        "num_hidden_layers": 24,
        "num_attention_heads": 32,
    },
}

class TTTConfig(PretrainedConfig):
    model_type = "ttt"
    
    def __init__(
        self,
        vocab_size=32000,
        hidden_size=2048,
        intermediate_size=5504,
        num_hidden_layers=24,
        num_attention_heads=32,
        hidden_act="silu",
        max_position_embeddings=1024,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=False,
        pad_token_id=None,
        bos_token_id=1,
        eos_token_id=2,
        pretraining_tp=1,
        tie_word_embeddings=True,
        rope_theta=10000.0,
        use_gate=False,
        share_qk=False,
        ttt_layer_type="linear",
        ttt_base_lr=1.0,
        mini_batch_size=16,
        pre_conv=False,
        conv_kernel=4,
        scan_checkpoint_group_size=0,
        mlp_ratio=14,  # ğŸ¯ æ–°å¢ï¼šMLP å€æ•¸åƒæ•¸ï¼Œç”¨æ–¼å…§éƒ¨å„ªåŒ–
        **kwargs,
    ):
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps
        self.pretraining_tp = pretraining_tp
        self.use_cache = use_cache
        self.rope_theta = rope_theta
        self.use_gate = use_gate
        self.share_qk = share_qk
        self.ttt_layer_type = ttt_layer_type
        self.ttt_base_lr = ttt_base_lr
        self.mini_batch_size = min(mini_batch_size, num_attention_heads)
        self.pre_conv = pre_conv
        self.conv_kernel = conv_kernel
        self.scan_checkpoint_group_size = scan_checkpoint_group_size
        self.mlp_ratio = mlp_ratio  # ğŸ¯ MLP å€æ•¸åƒæ•¸
        
        super().__init__(
            pad_token_id=pad_token_id,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            tie_word_embeddings=tie_word_embeddings,
            **kwargs,
        )

class TTTLinear(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        # ç¢ºä¿ head_dim æ˜¯æ•´æ•¸ä¸” num_heads * head_dim = hidden_size
        self.head_dim = self.hidden_size // self.num_heads
        assert self.num_heads * self.head_dim == self.hidden_size, \
            f"hidden_size {self.hidden_size} must be divisible by num_heads {self.num_heads}"
        self.mini_batch_size = config.mini_batch_size
        
        # æ ¸å¿ƒç·šæ€§å±¤ - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´
        self.wq = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)  # å…±ç”¨ Q/K æŠ•å½±
        self.wv = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)  # V å°ˆç”¨æŠ•å½±
        self.wo = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        
        # TTT æ ¸å¿ƒåƒæ•¸ - èˆ‡ kernels ç‰ˆæœ¬å®Œå…¨ä¸€è‡´
        self.W1 = nn.Parameter(
            torch.normal(0, 0.02, size=(self.num_heads, self.head_dim, self.head_dim))
        )
        self.b1 = nn.Parameter(torch.zeros(self.num_heads, 1, self.head_dim))
        
        # ğŸ”§ æ–°å¢ï¼šèˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´çš„æ¨ç†åƒæ•¸
        # token_idx ç·©è¡å€å’Œå¯å­¸ç¿’åç½®
        token_idx = 1. / torch.arange(1, self.mini_batch_size + 1).reshape(1, 1, -1, 1)
        self.register_buffer('token_idx', token_idx, persistent=False)
        self.learnable_token_idx_bias = nn.Parameter(torch.zeros((1, 1, self.mini_batch_size, 1)))
        
        # å¯å­¸ç¿’çš„ TTT å­¸ç¿’ç‡æŠ•å½±å’Œåç½®
        self.qkv_learnable_ttt_lr_proj = nn.Linear(
            self.hidden_size, 
            3 * self.hidden_size + self.num_heads, 
            bias=False
        )
        self.learnable_ttt_lr_bias = nn.Parameter(torch.zeros(1, 1, self.num_heads))
        
        # ğŸ”§ æ–°å¢ï¼šåˆ†é›¢çš„ TTT LayerNorm åƒæ•¸ (èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´)
        ttt_norm_weight = torch.ones(self.head_dim)
        ttt_norm_bias = torch.zeros(self.head_dim)
        # [1,nh,1,f] æ ¼å¼ï¼Œèˆ‡ kernels ç‰ˆæœ¬å®Œå…¨ä¸€è‡´
        self.ttt_norm_weight = nn.Parameter(
            ttt_norm_weight.reshape(1, 1, 1, -1).expand(1, self.num_heads, 1, -1).contiguous()
        )
        self.ttt_norm_bias = nn.Parameter(
            ttt_norm_bias.reshape(1, 1, 1, -1).expand(1, self.num_heads, 1, -1).contiguous()
        )
        
        # å±¤æ­£è¦åŒ– - ç”¨æ–¼å¾Œè™•ç†
        self.post_norm = nn.LayerNorm(self.hidden_size, eps=1e-6)
        
        # TTT LayerNorm - èˆ‡ JAX main ä¸€è‡´
        self.ttt_norm = self._ttt_layer_norm
        
        # é–€æ§æ©Ÿåˆ¶ - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´
        self.wg = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
        
        # Pre-convolution - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´ï¼šQ å’Œ K åˆ†é›¢å·ç©
        if config.pre_conv:
            self.conv_q = nn.Conv1d(
                in_channels=self.hidden_size,
                out_channels=self.hidden_size,
                kernel_size=config.conv_kernel,
                padding=config.conv_kernel - 1,
                groups=self.hidden_size,  # depthwise convolution
                bias=True,  # kernelsç‰ˆæœ¬æœ‰bias
            )
            self.conv_k = nn.Conv1d(
                in_channels=self.hidden_size,
                out_channels=self.hidden_size,
                kernel_size=config.conv_kernel,
                padding=config.conv_kernel - 1,
                groups=self.hidden_size,  # depthwise convolution
                bias=True,  # kernelsç‰ˆæœ¬æœ‰bias
            )

    def _ttt_layer_norm(self, x):
        """TTT LayerNorm - èˆ‡ JAX main ä¸€è‡´çš„å¯¦ç¾"""
        # x: [B*nh, N, f] -> [B*nh, N, f]
        # é‡å¡‘ç‚º [B*nh, N, 1, f] ä»¥åŒ¹é…åƒæ•¸æ ¼å¼
        x_reshaped = x.unsqueeze(2)  # [B*nh, N, 1, f]
        
        # è¨ˆç®—å‡å€¼å’Œæ–¹å·®
        mean = x_reshaped.mean(dim=-1, keepdim=True)  # [B*nh, N, 1, 1]
        var = x_reshaped.var(dim=-1, keepdim=True, unbiased=False)  # [B*nh, N, 1, 1]
        std = torch.sqrt(var + 1e-6)
        
        # æ¨™æº–åŒ–
        x_norm = (x_reshaped - mean) / std  # [B*nh, N, 1, f]
        
        # æ‡‰ç”¨å¯å­¸ç¿’åƒæ•¸
        # self.ttt_norm_weight: [1, nh, 1, f] -> [B*nh, 1, 1, f]
        # self.ttt_norm_bias: [1, nh, 1, f] -> [B*nh, 1, 1, f]
        weight = self.ttt_norm_weight.expand(x_norm.size(0) // self.num_heads, self.num_heads, 1, -1).reshape(-1, 1, 1, self.head_dim)
        bias = self.ttt_norm_bias.expand(x_norm.size(0) // self.num_heads, self.num_heads, 1, -1).reshape(-1, 1, 1, self.head_dim)
        
        x_out = weight * x_norm + bias  # [B*nh, N, 1, f]
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        return x_out.squeeze(2)  # [B*nh, N, f]

    def get_QKV_ttt_lr(self, hidden_states):
        """èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´çš„ QKV å’Œ TTT å­¸ç¿’ç‡è¨ˆç®—"""
        B, N, D = hidden_states.shape

        XQKV_ttt_lr = self.qkv_learnable_ttt_lr_proj(hidden_states)  # [B,N, 3*F + nh]
        XQKV, ttt_lr = torch.split(XQKV_ttt_lr, split_size_or_sections=[3 * D, self.num_heads], dim=-1)

        # ttt_lr = (fixed ttt base lr) * (learnable lr multiplier)
        ttt_lr = self.config.ttt_base_lr * torch.sigmoid(
            (ttt_lr + self.learnable_ttt_lr_bias).permute(0, 2, 1).reshape(-1, N, 1)
        ) / self.head_dim  # ([B*nh,N,1] + [1,1,nh]) -> [B*nh,N,1]

        XQK, XGate, XV = torch.split(XQKV, split_size_or_sections=self.hidden_size, dim=-1)  # [B,N,D]
        XV = XV.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3).reshape(-1, N, self.head_dim)

        return XQK, XV, XGate, ttt_lr

    def conv_qk_fused(self, XQK, is_prefill=True):
        """èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´çš„å·ç©è™•ç†"""
        if not self.config.pre_conv or not hasattr(self, 'conv_q'):
            # æ²’æœ‰å·ç©æ™‚ï¼ŒQ å’Œ K éƒ½ä½¿ç”¨å…±ç”¨æŠ•å½±
            return XQK, XQK
            
        B, N, D = XQK.shape
        # Conv1d éœ€è¦ (batch, channels, seq_len) æ ¼å¼
        XQK_conv = XQK.transpose(1, 2)  # [B, D, N]
        
        # åˆ†åˆ¥å° Q å’Œ K é€²è¡Œå·ç©
        XQ_conv = self.conv_q(XQK_conv)
        XK_conv = self.conv_k(XQK_conv)
        
        if is_prefill:
            # å»é™¤å³å´ padding
            XQ_conv = XQ_conv[:, :, :N]
            XK_conv = XK_conv[:, :, :N]
        
        XQ = XQ_conv.transpose(1, 2)  # [B, N, D]
        XK = XK_conv.transpose(1, 2)  # [B, N, D]
        
        return XQ, XK

    def get_eta(self, hidden_states, ttt_lr_mult=1.0):
        """è¨ˆç®—å­¸ç¿’ç‡èª¿æ•´å› å­ - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´"""
        B, N, _ = hidden_states.shape
        
        # ç²å– token_idx å’Œå¯å­¸ç¿’åç½®
        token_idx = self.token_idx + self.learnable_token_idx_bias
        token_idx = torch.clamp(token_idx, min=0.0)  # ç¢ºä¿å¤§æ–¼0
        
        # ç²å–å¯å­¸ç¿’çš„ TTT å­¸ç¿’ç‡
        _, _, _, ttt_lr = self.get_QKV_ttt_lr(hidden_states)  # [B*nh,N,1]
        
        # è¨ˆç®—æœ€çµ‚çš„å­¸ç¿’ç‡
        eta = ttt_lr_mult * token_idx * ttt_lr  # [B*nh,N,1] * [1,1,mini_batch,1]
        
        return eta



    def forward(self, hidden_states, input_ids=None, position_ids=None, 
                deterministic=True, output_ttt_stats=False, ttt_lr_mult=1.0,
                iteration_counts=None, max_iter=None):
        """å®Œæ•´çš„ TTT å‰å‘å‚³æ’­ï¼ŒåŒ…å« mini-batch è¿­ä»£æ›´æ–° - èˆ‡ JAX ç‰ˆæœ¬å®Œå…¨ä¸€è‡´"""
        batch_size, seq_length = hidden_states.shape[:2]
        
        # ğŸ”§ ä½¿ç”¨ kernels ç‰ˆæœ¬çš„ QKV å’Œå­¸ç¿’ç‡è¨ˆç®—
        XQK, XV, XGate, ttt_lr = self.get_QKV_ttt_lr(hidden_states)
        
        # ğŸ”§ ä½¿ç”¨ kernels ç‰ˆæœ¬çš„å·ç©è™•ç†
        XQ, XK = self.conv_qk_fused(XQK, is_prefill=True)
        
        # é‡å¡‘ç‚ºå¤šé ­å½¢å¼ - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´
        XQ = XQ.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        XK = XK.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        
        # å±•å¹³ç‚º kernels æ ¼å¼ï¼š[B*nh, N, f]
        XQ = XQ.reshape(-1, seq_length, self.head_dim).contiguous()
        XK = XK.reshape(-1, seq_length, self.head_dim).contiguous()
        XV = XV.contiguous()
        
        # ğŸ”§ è¨ˆç®— token_idx å’Œ eta - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´
        token_idx = self.token_idx + self.learnable_token_idx_bias
        token_idx = torch.clamp(token_idx, min=0.0)
        
        # ğŸ”§ å®Œæ•´çš„ TTT è™•ç† (åŒ…å« mini-batch è¿­ä»£)
        B_mul_NH, N, HF = XV.shape
        
        # ä½¿ç”¨å¯è¨“ç·´çš„ W1, b1 åƒæ•¸
        W1_init = self.W1.unsqueeze(0).expand(batch_size, -1, -1, -1).reshape(B_mul_NH, HF, HF)
        b1_init = self.b1.unsqueeze(0).expand(batch_size, -1, -1, -1).reshape(B_mul_NH, 1, HF)
        
        # ğŸ”§ å¯¦ç¾å®Œæ•´çš„ mini-batch TTT (èˆ‡ JAX ç‰ˆæœ¬ä¸€è‡´)
        mini_batch_size = min(self.mini_batch_size, N)
        num_mini_batches = (N + mini_batch_size - 1) // mini_batch_size
        
        # åˆå§‹åŒ– TTT åƒæ•¸
        ttt_params_init = (W1_init, b1_init)
        ttt_params_mini_batch_init = ttt_params_init
        
        # å­˜å„²æ¯å€‹ mini-batch çš„è¼¸å‡º
        outputs = []
        ttt_stats_list = []
        
        for i in range(num_mini_batches):
            start_idx = i * mini_batch_size
            end_idx = min((i + 1) * mini_batch_size, N)
            
            # æå–ç•¶å‰ mini-batch çš„æ•¸æ“š
            XQ_mini = XQ[:, start_idx:end_idx, :]  # [B*nh, mini_batch_size, f]
            XK_mini = XK[:, start_idx:end_idx, :]  # [B*nh, mini_batch_size, f]
            XV_mini = XV[:, start_idx:end_idx, :]  # [B*nh, mini_batch_size, f]
            
            # è¨ˆç®—ç•¶å‰ mini-batch çš„å­¸ç¿’ç‡
            eta_mini = ttt_lr[:, start_idx:end_idx, :] * ttt_lr_mult  # [B*nh, mini_batch_size, 1]
            
            # è™•ç†ç•¶å‰ mini-batch (èˆ‡ JAX ç‰ˆæœ¬ä¸€è‡´)
            output_mini_batch, ttt_stats, ttt_params_mini_batch_new = self._process_mini_batch(
                XQ_mini, XK_mini, XV_mini, eta_mini, 
                ttt_params_init, ttt_params_mini_batch_init, None  # ttt_norm_params åœ¨ PyTorch ä¸­æ˜¯å…§å»ºçš„
            )
            
            outputs.append(output_mini_batch)
            ttt_stats_list.append(ttt_stats)
            
            # æ›´æ–° TTT åƒæ•¸ (èˆ‡ JAX ç‰ˆæœ¬ä¸€è‡´)
            # å°‡æ›´æ–°çš„åƒæ•¸å‚³éçµ¦ä¸‹ä¸€å€‹ mini-batch
            if i < num_mini_batches - 1:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹ mini-batch
                ttt_params_mini_batch_init = ttt_params_mini_batch_new
        
        # åˆä½µæ‰€æœ‰ mini-batch çš„è¼¸å‡º
        output = torch.cat(outputs, dim=1)  # [B*nh, N, f]
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        output = output.reshape(batch_size, self.num_heads, seq_length, self.head_dim)
        output = output.permute(0, 2, 1, 3).reshape(batch_size, seq_length, -1)
        
        # æ‡‰ç”¨é–€æ§ - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´ä½¿ç”¨ GELU
        gate = F.gelu(XGate)
        output = gate * self.post_norm(output)
        
        # æœ€çµ‚æŠ•å½±
        output = self.wo(output)
        
        return output
    
    def _process_mini_batch(self, XQ_mini, XK_mini, XV_mini, eta_mini, ttt_params_init, ttt_params_mini_batch_init, ttt_norm_params):
        """è™•ç†å–®å€‹ mini-batch çš„ TTT æ›´æ–° - ç©©å®šæ€§æ”¹é€²ç‰ˆæœ¬"""
        W1_init, b1_init = ttt_params_mini_batch_init  # ä½¿ç”¨ mini_batch åˆå§‹åƒæ•¸
        mini_batch_size = XK_mini.shape[1]
        
        # 1. æå–å­¸ç¿’ç‡ä¸¦æ·»åŠ ç©©å®šæ€§ç´„æŸ
        square_eta_mini_batch = eta_mini  # [B*nh, mini_batch_size, 1]
        # ä¿®æ­£ï¼šç¢ºä¿ last_eta_in_mini_batch çš„ç¶­åº¦æ­£ç¢º
        last_eta_in_mini_batch = eta_mini[:, -1:, :]  # [B*nh, 1, 1]
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šé™åˆ¶å­¸ç¿’ç‡ç¯„åœ
        square_eta_mini_batch = torch.clamp(square_eta_mini_batch, min=1e-6, max=1.0)
        last_eta_in_mini_batch = torch.clamp(last_eta_in_mini_batch, min=1e-6, max=1.0)
        
        # 2. å‰å‘å‚³æ’­
        X1 = XK_mini  # [B*nh, mini_batch_size, f]
        Z1 = X1 @ W1_init + b1_init  # [B*nh, mini_batch_size, f]
        
        # 3. TTT LayerNorm (ç©©å®šæ€§æ”¹é€²)
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šä½¿ç”¨æ›´ç©©å®šçš„ LayerNorm å¯¦ç¾
        mu = Z1.mean(dim=-1, keepdim=True)
        var = Z1.var(dim=-1, keepdim=True, unbiased=False)
        std = torch.sqrt(var + 1e-5)  # å¢åŠ  epsilon å€¼
        Z1_hat = (Z1 - mu) / std
        
        # æ‡‰ç”¨ TTT LayerNorm æ¬Šé‡å’Œåç½®
        NH = self.num_heads
        B_mul_NH, seq_len, hidden_dim = Z1_hat.shape
        batch_size = B_mul_NH // NH
        
        # æ˜ç¢ºæŒ‡å®š reshape çš„ç¶­åº¦ï¼Œé¿å…è‡ªå‹•æ¨æ–·éŒ¯èª¤
        Z1_hat_reshaped = Z1_hat.reshape(batch_size, NH, seq_len, hidden_dim)
        ttt_norm_out = (self.ttt_norm_weight * Z1_hat_reshaped + 
                        self.ttt_norm_bias).reshape(B_mul_NH, seq_len, hidden_dim)
        
        # 4. è¨ˆç®— SSL ç›®æ¨™å’Œæ¢¯åº¦
        ssl_target = XV_mini - XK_mini  # è‡ªç›£ç£å­¸ç¿’ç›®æ¨™
        grad_l_wrt_ttt_norm_out = ttt_norm_out - ssl_target
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šæ¢¯åº¦è£å‰ª
        grad_l_wrt_ttt_norm_out = torch.clamp(grad_l_wrt_ttt_norm_out, min=-10.0, max=10.0)
        
        # 5. è¨ˆç®—æ¢¯åº¦ (ç©©å®šæ€§æ”¹é€²)
        grad_l_wrt_Z1 = self._compute_grad_wrt_Z1_stable(Z1, grad_l_wrt_ttt_norm_out)
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šæ¢¯åº¦è£å‰ª
        grad_l_wrt_Z1 = torch.clamp(grad_l_wrt_Z1, min=-5.0, max=5.0)
        
        # 6. è¨ˆç®— TTT çµ±è¨ˆä¿¡æ¯
        ttt_loss_mse_step_0 = None
        ttt_loss_mse_init = None
        ttt_loss_mse_step_1 = None
        
        if hasattr(self, 'config') and hasattr(self.config, 'output_ttt_stats') and self.config.output_ttt_stats:
            ttt_loss_mse_step_0 = (grad_l_wrt_ttt_norm_out[-1] ** 2).mean()
            
            # è¨ˆç®—ä½¿ç”¨æ•´å€‹åºåˆ—åˆå§‹åƒæ•¸çš„æå¤±
            W1_0, b1_0 = ttt_params_init  # ä½¿ç”¨æ•´å€‹åºåˆ—çš„åˆå§‹åƒæ•¸
            Z1_0 = X1 @ W1_0 + b1_0
            mu_0 = Z1_0.mean(dim=-1, keepdim=True)
            var_0 = Z1_0.var(dim=-1, keepdim=True, unbiased=False)
            std_0 = torch.sqrt(var_0 + 1e-5)
            Z1_hat_0 = (Z1_0 - mu_0) / std_0
            B_mul_NH_0, seq_len_0, hidden_dim_0 = Z1_hat_0.shape
            batch_size_0 = B_mul_NH_0 // NH
            
            # æ˜ç¢ºæŒ‡å®š reshape çš„ç¶­åº¦
            Z1_hat_0_reshaped = Z1_hat_0.reshape(batch_size_0, NH, seq_len_0, hidden_dim_0)
            ttt_norm_out_0 = (self.ttt_norm_weight * Z1_hat_0_reshaped + 
                              self.ttt_norm_bias).reshape(B_mul_NH_0, seq_len_0, hidden_dim_0)
            ttt_loss_mse_init = ((ttt_norm_out_0 - ssl_target)[-1] ** 2).mean()
        
        # 7. è¨ˆç®—æ›´æ–°å¾Œçš„è¼¸å‡º (ç©©å®šæ€§æ”¹é€²)
        X1_bar = XQ_mini  # Query ä½œç‚ºè¼¸å…¥
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šç°¡åŒ–æ³¨æ„åŠ›è¨ˆç®—ï¼Œé¿å…å¤§çŸ©é™£é‹ç®—
        # è¨ˆç®—æ³¨æ„åŠ›çŸ©é™£ (ä¸‹ä¸‰è§’çŸ©é™£)
        Attn1 = torch.tril(X1_bar @ X1.transpose(-2, -1))  # [B*nh, mini_batch_size, mini_batch_size]
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šé™åˆ¶æ³¨æ„åŠ›æ¬Šé‡ç¯„åœ
        Attn1 = torch.clamp(Attn1, min=-5.0, max=5.0)
        
        # è¨ˆç®—æ›´æ–°çš„åç½®
        ones_matrix = torch.ones_like(Attn1)
        b1_bar = b1_init - (square_eta_mini_batch * torch.tril(ones_matrix)) @ grad_l_wrt_Z1
        
        # è¨ˆç®—æ›´æ–°çš„è¼¸å‡º
        Z1_bar = X1_bar @ W1_init - (square_eta_mini_batch * Attn1) @ grad_l_wrt_Z1 + b1_bar
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šé™åˆ¶ä¸­é–“çµæœç¯„åœ
        Z1_bar = torch.clamp(Z1_bar, min=-10.0, max=10.0)
        
        # å°æ›´æ–°å¾Œçš„è¼¸å‡ºæ‡‰ç”¨ LayerNorm
        mu_bar = Z1_bar.mean(dim=-1, keepdim=True)
        var_bar = Z1_bar.var(dim=-1, keepdim=True, unbiased=False)
        std_bar = torch.sqrt(var_bar + 1e-5)
        Z1_hat_bar = (Z1_bar - mu_bar) / std_bar
        B_mul_NH_bar, seq_len_bar, hidden_dim_bar = Z1_hat_bar.shape
        batch_size_bar = B_mul_NH_bar // NH
        
        # æ˜ç¢ºæŒ‡å®š reshape çš„ç¶­åº¦
        Z1_hat_bar_reshaped = Z1_hat_bar.reshape(batch_size_bar, NH, seq_len_bar, hidden_dim_bar)
        ttt_norm_out_bar = (self.ttt_norm_weight * Z1_hat_bar_reshaped + 
                            self.ttt_norm_bias).reshape(B_mul_NH_bar, seq_len_bar, hidden_dim_bar)
        
        # 8. æ®˜å·®é€£æ¥: f(x) = x + LN(f_res(x))
        output_mini_batch = X1_bar + ttt_norm_out_bar
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šé™åˆ¶æœ€çµ‚è¼¸å‡ºç¯„åœ
        output_mini_batch = torch.clamp(output_mini_batch, min=-10.0, max=10.0)
        
        # 9. æ›´æ–° TTT åƒæ•¸ (ç©©å®šæ€§æ”¹é€²)
        W1_bar_last = W1_init - (last_eta_in_mini_batch * X1).transpose(-2, -1) @ grad_l_wrt_Z1
        b1_bar_last = b1_init - torch.sum(last_eta_in_mini_batch * grad_l_wrt_Z1, dim=0, keepdim=True)
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šåƒæ•¸ç¯„åœé™åˆ¶
        W1_bar_last = torch.clamp(W1_bar_last, min=-2.0, max=2.0)
        b1_bar_last = torch.clamp(b1_bar_last, min=-2.0, max=2.0)
        
        # 10. è¨ˆç®—æ›´æ–°å¾Œåƒæ•¸çš„æå¤±
        if hasattr(self, 'config') and hasattr(self.config, 'output_ttt_stats') and self.config.output_ttt_stats:
            X1_last_fwd_new = X1[-1:] @ W1_bar_last + b1_bar_last
            mu_new = X1_last_fwd_new.mean(dim=-1, keepdim=True)
            var_new = X1_last_fwd_new.var(dim=-1, keepdim=True, unbiased=False)
            std_new = torch.sqrt(var_new + 1e-5)
            Z1_hat_new = (X1_last_fwd_new - mu_new) / std_new
            B_mul_NH_new, seq_len_new, hidden_dim_new = Z1_hat_new.shape
            batch_size_new = B_mul_NH_new // NH
            
            # æ˜ç¢ºæŒ‡å®š reshape çš„ç¶­åº¦
            Z1_hat_new_reshaped = Z1_hat_new.reshape(batch_size_new, NH, seq_len_new, hidden_dim_new)
            X1_last_fwd_new_norm = (self.ttt_norm_weight * Z1_hat_new_reshaped + 
                                    self.ttt_norm_bias).reshape(B_mul_NH_new, seq_len_new, hidden_dim_new)
            ttt_loss_mse_step_1 = ((X1_last_fwd_new_norm - ssl_target[-1:]) ** 2).mean()
        
        # 11. è¿”å›æ›´æ–°çš„åƒæ•¸å’Œè¼¸å‡º
        ttt_params_mini_batch_new = (W1_bar_last, b1_bar_last)
        ttt_stats = (ttt_loss_mse_init, ttt_loss_mse_step_0, ttt_loss_mse_step_1)
        
        return output_mini_batch, ttt_stats, ttt_params_mini_batch_new
    
    def _compute_grad_wrt_Z1_stable(self, Z1, grad_l_wrt_ttt_norm_out):
        """è¨ˆç®— grad_l_wrt_Z1 - ç©©å®šæ€§æ”¹é€²ç‰ˆæœ¬"""
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šç°¡åŒ–çš„æ¢¯åº¦è¨ˆç®—
        # é€™è£¡æˆ‘å€‘ä½¿ç”¨ä¸€å€‹æ›´ç©©å®šçš„è¿‘ä¼¼
        batch_size, seq_len, hidden_dim = Z1.shape
        
        # è¨ˆç®— LayerNorm çš„æ¢¯åº¦è¿‘ä¼¼
        mu = Z1.mean(dim=-1, keepdim=True)
        var = Z1.var(dim=-1, keepdim=True, unbiased=False)
        std = torch.sqrt(var + 1e-5)
        
        # ç°¡åŒ–çš„æ¢¯åº¦è¨ˆç®—
        grad_Z1 = grad_l_wrt_ttt_norm_out / (std + 1e-5)
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šæ¢¯åº¦è£å‰ª
        grad_Z1 = torch.clamp(grad_Z1, min=-5.0, max=5.0)
        
        return grad_Z1

    def forward_optimized(self, hidden_states, input_ids=None, position_ids=None, 
                          deterministic=True, output_ttt_stats=False, ttt_lr_mult=1.0):
        """å„ªåŒ–çš„ TTT å‰å‘å‚³æ’­ - æé«˜ GPU åˆ©ç”¨ç‡"""
        batch_size, seq_length = hidden_states.shape[:2]
        
        # ğŸ”§ ä½¿ç”¨ kernels ç‰ˆæœ¬çš„ QKV å’Œå­¸ç¿’ç‡è¨ˆç®—
        XQK, XV, XGate, ttt_lr = self.get_QKV_ttt_lr(hidden_states)
        
        # ğŸ”§ ä½¿ç”¨ kernels ç‰ˆæœ¬çš„å·ç©è™•ç†
        XQ, XK = self.conv_qk_fused(XQK, is_prefill=True)
        
        # é‡å¡‘ç‚ºå¤šé ­å½¢å¼ - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´
        XQ = XQ.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        XK = XK.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        
        # å±•å¹³ç‚º kernels æ ¼å¼ï¼š[B*nh, N, f]
        XQ = XQ.reshape(-1, seq_length, self.head_dim).contiguous()
        XK = XK.reshape(-1, seq_length, self.head_dim).contiguous()
        XV = XV.contiguous()
        
        # ğŸ”§ å„ªåŒ–ï¼šä½¿ç”¨æ›´å¤§çš„ mini-batch æˆ–ä¸¦è¡Œè™•ç†
        B_mul_NH, N, HF = XV.shape
        
        # ä½¿ç”¨å¯è¨“ç·´çš„ W1, b1 åƒæ•¸
        W1_init = self.W1.unsqueeze(0).expand(batch_size, -1, -1, -1).reshape(B_mul_NH, HF, HF)
        b1_init = self.b1.unsqueeze(0).expand(batch_size, -1, -1, -1).reshape(B_mul_NH, 1, HF)
        
        # ğŸ”§ å„ªåŒ–ï¼šä½¿ç”¨å¹³è¡¡çš„ mini-batch å¤§å°
        optimized_mini_batch_size = min(8, N)  # å¹³è¡¡ç‰ˆæœ¬ï¼šä½¿ç”¨ 8
        num_mini_batches = (N + optimized_mini_batch_size - 1) // optimized_mini_batch_size
        
        # åˆå§‹åŒ– TTT åƒæ•¸
        ttt_params_init = (W1_init, b1_init)
        ttt_params_mini_batch_init = ttt_params_init
        
        # ğŸ”§ å„ªåŒ–ï¼šé åˆ†é…è¼¸å‡ºå¼µé‡ï¼Œæ¸›å°‘å…§å­˜åˆ†é…
        output = torch.zeros_like(XQ)
        
        for i in range(num_mini_batches):
            start_idx = i * optimized_mini_batch_size
            end_idx = min((i + 1) * optimized_mini_batch_size, N)
            
            # æå–ç•¶å‰ mini-batch çš„æ•¸æ“š
            XQ_mini = XQ[:, start_idx:end_idx, :]
            XK_mini = XK[:, start_idx:end_idx, :]
            XV_mini = XV[:, start_idx:end_idx, :]
            
            # è¨ˆç®—ç•¶å‰ mini-batch çš„å­¸ç¿’ç‡
            eta_mini = ttt_lr[:, start_idx:end_idx, :] * ttt_lr_mult
            
            # è™•ç†ç•¶å‰ mini-batch
            output_mini_batch, ttt_stats, ttt_params_mini_batch_new = self._process_mini_batch(
                XQ_mini, XK_mini, XV_mini, eta_mini, 
                ttt_params_init, ttt_params_mini_batch_init, None
            )
            
            # ğŸ”§ å„ªåŒ–ï¼šç›´æ¥å¯«å…¥é åˆ†é…çš„å¼µé‡
            output[:, start_idx:end_idx, :] = output_mini_batch
            
            # æ›´æ–° TTT åƒæ•¸
            if i < num_mini_batches - 1:
                ttt_params_mini_batch_init = ttt_params_mini_batch_new
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        output = output.reshape(batch_size, self.num_heads, seq_length, self.head_dim)
        output = output.permute(0, 2, 1, 3).reshape(batch_size, seq_length, -1)
        
        # æ‡‰ç”¨é–€æ§
        gate = F.gelu(XGate)
        output = gate * self.post_norm(output)
        
        # æœ€çµ‚æŠ•å½±
        output = self.wo(output)
        
        return output

def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors."""
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

class BFloat16LayerNorm(nn.Module):
    """LayerNorm å…¼å®¹ BFloat16 çš„ç‰ˆæœ¬"""
    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        if self.elementwise_affine:
            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.bias = nn.Parameter(torch.zeros(normalized_shape))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)

    def forward(self, x):
        # ä¿å­˜åŸå§‹æ•¸æ“šé¡å‹
        original_dtype = x.dtype
        # è½‰æ›ç‚º float32 é€²è¡Œè¨ˆç®—
        x_float = x.float()
        # åŸ·è¡Œ LayerNorm
        mean = x_float.mean(-1, keepdim=True)
        var = ((x_float - mean) ** 2).mean(-1, keepdim=True)
        x_norm = (x_float - mean) / torch.sqrt(var + self.eps)
        # æ‡‰ç”¨æ¬Šé‡å’Œåç½®
        if self.elementwise_affine:
            x_norm = x_norm * self.weight.float() + self.bias.float()
        # è½‰å›åŸå§‹æ•¸æ“šé¡å‹
        return x_norm.to(original_dtype)

class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6, elementwise_affine=True):
        super().__init__()
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        if self.elementwise_affine:
            self.weight = nn.Parameter(torch.ones(dim))
        else:
            self.register_parameter('weight', None)

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        if self.weight is not None:
            output = output * self.weight
        return output

class TTTBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attn = TTTLinear(config)
        self.ln_2 = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # MLP - ä½¿ç”¨å¯é…ç½®çš„ mlp_ratio
        mlp_ratio = getattr(config, 'mlp_ratio', 14)  # ğŸ¯ å¾é…ç½®ä¸­ç²å– MLP å€æ•¸
        multiple_of = 256
        mlp_hidden = int(config.hidden_size * mlp_ratio * 2 / 3)
        mlp_hidden = multiple_of * ((mlp_hidden + multiple_of - 1) // multiple_of)
        self.w1 = nn.Linear(config.hidden_size, mlp_hidden, bias=False)
        self.w2 = nn.Linear(mlp_hidden, config.hidden_size, bias=False)
        self.w3 = nn.Linear(config.hidden_size, mlp_hidden, bias=False)
        self.mlp = lambda x: self.w2(F.silu(self.w1(x)) * self.w3(x))

    def forward(self, x, ttt_lr_mult=1.0):
        # ğŸ”§ ä½¿ç”¨å„ªåŒ–ç‰ˆæœ¬æé«˜ GPU åˆ©ç”¨ç‡
        x = x + self.attn.forward_optimized(self.ln_1(x), ttt_lr_mult=ttt_lr_mult)
        x = x + self.mlp(self.ln_2(x))
        return x

class TTTPreTrainedModel(PreTrainedModel):
    config_class = TTTConfig
    supports_gradient_checkpointing = True
    _no_split_modules = ["TTTBlock"]

    def __init__(self, *inputs, **kwargs):
        super().__init__(*inputs, **kwargs)

class TTTModel(TTTPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.config = config

        self.wte = nn.Embedding(config.vocab_size, config.hidden_size)
        self.h = nn.ModuleList([TTTBlock(config) for _ in range(config.num_hidden_layers)])
        self.ln_f = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = True,
        ttt_lr_mult: float = 1.0,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        hidden_states = self.wte(input_ids)
        
        for block in self.h:
            hidden_states = block(hidden_states, ttt_lr_mult=ttt_lr_mult)
        
        hidden_states = self.ln_f(hidden_states)
        
        return BaseModelOutputWithPast(
            past_key_values=None,
            last_hidden_state=hidden_states,
        )

class TTT(TTTPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.config = config

        self.transformer = TTTModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.n_layer = config.num_hidden_layers

        # åˆå§‹åŒ–æ¬Šé‡
        self.apply(self._init_weights)
        self.post_init()
        self.lm_head.weight = self.transformer.wte.weight

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm) or isinstance(module, RMSNorm):
            if hasattr(module, "bias") and module.bias is not None:
                torch.nn.init.zeros_(module.bias)
            if hasattr(module, "weight") and module.weight is not None:
                torch.nn.init.ones_(module.weight)

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        labels: Optional[torch.LongTensor] = None,
        loss_masks: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = True,
        ttt_lr_mult: float = 1.0,
        return_hidden: bool = False,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        transformer_outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            ttt_lr_mult=ttt_lr_mult,
        )
        hidden_states = transformer_outputs.last_hidden_state
        logits = self.lm_head(hidden_states)

        if labels is not None:
            logits_float = logits.to(torch.float32)
            labels_long = labels.to(torch.long)
            if loss_masks is None:
                loss_masks = torch.ones_like(labels_long, dtype=torch.float32)
            loss_masks = loss_masks.to(torch.float32)
            
            # å‰µå»ºæœ‰æ•ˆçš„æ¨™ç±¤é®ç½©ï¼Œæ’é™¤ -100
            valid_labels_mask = (labels_long != -100)
            
            # åªå°æœ‰æ•ˆçš„æ¨™ç±¤è¨ˆç®—æå¤±
            if valid_labels_mask.any():
                # ç²å–æœ‰æ•ˆçš„æ¨™ç±¤å’Œå°æ‡‰çš„ logits
                valid_labels = labels_long[valid_labels_mask]
                valid_logits = logits_float[valid_labels_mask]
                
                # è¨ˆç®— log probabilities
                log_probs = F.log_softmax(valid_logits, dim=-1)
                token_log_prob = torch.gather(log_probs, -1, valid_labels.unsqueeze(-1)).squeeze(-1)
                
                # è¨ˆç®—æå¤±
                loss = -torch.mean(token_log_prob)
            else:
                # å¦‚æœæ²’æœ‰æœ‰æ•ˆæ¨™ç±¤ï¼Œè¿”å›é›¶æå¤±
                loss = torch.tensor(0.0, device=logits.device, requires_grad=True)
        else:
            loss = None
        if return_hidden:
            return logits, loss, hidden_states
        else:
            return logits, loss, None

    def forward_simple(self, hidden_states, input_ids=None, position_ids=None, 
                       deterministic=True, output_ttt_stats=False, ttt_lr_mult=1.0):
        """ç°¡åŒ–çš„ TTT å‰å‘å‚³æ’­ - æ›´ç©©å®šçš„ç‰ˆæœ¬ï¼Œé¿å…è¤‡é›œçš„ mini-batch è¿­ä»£"""
        batch_size, seq_length = hidden_states.shape[:2]
        
        # ğŸ”§ ä½¿ç”¨ kernels ç‰ˆæœ¬çš„ QKV å’Œå­¸ç¿’ç‡è¨ˆç®—
        XQK, XV, XGate, ttt_lr = self.get_QKV_ttt_lr(hidden_states)
        
        # ğŸ”§ ä½¿ç”¨ kernels ç‰ˆæœ¬çš„å·ç©è™•ç†
        XQ, XK = self.conv_qk_fused(XQK, is_prefill=True)
        
        # é‡å¡‘ç‚ºå¤šé ­å½¢å¼ - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´
        XQ = XQ.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        XK = XK.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        
        # å±•å¹³ç‚º kernels æ ¼å¼ï¼š[B*nh, N, f]
        XQ = XQ.reshape(-1, seq_length, self.head_dim).contiguous()
        XK = XK.reshape(-1, seq_length, self.head_dim).contiguous()
        XV = XV.contiguous()
        
        # ğŸ”§ ç°¡åŒ–çš„ TTT è™•ç† (å–®æ¬¡æ›´æ–°ï¼Œæ›´ç©©å®š)
        B_mul_NH, N, HF = XV.shape
        
        # ä½¿ç”¨å¯è¨“ç·´çš„ W1, b1 åƒæ•¸
        W1_init = self.W1.unsqueeze(0).expand(batch_size, -1, -1, -1).reshape(B_mul_NH, HF, HF)
        b1_init = self.b1.unsqueeze(0).expand(batch_size, -1, -1, -1).reshape(B_mul_NH, 1, HF)
        
        # ğŸ”§ ç°¡åŒ–çš„ TTT å¯¦ç¾ (å–®æ¬¡æ›´æ–°)
        # 1. å‰å‘å‚³æ’­
        Z1 = XK @ W1_init + b1_init  # [B*nh, N, f]
        
        # 2. TTT LayerNorm
        mu = Z1.mean(dim=-1, keepdim=True)
        var = Z1.var(dim=-1, keepdim=True, unbiased=False)
        std = torch.sqrt(var + 1e-5)
        Z1_hat = (Z1 - mu) / std
        
        # æ‡‰ç”¨ TTT LayerNorm æ¬Šé‡å’Œåç½®
        NH = self.num_heads
        ttt_norm_out = (self.ttt_norm_weight * Z1_hat.reshape(-1, NH, N, HF) + 
                        self.ttt_norm_bias).reshape(B_mul_NH, N, HF)
        
        # 3. è¨ˆç®— SSL ç›®æ¨™å’Œæ¢¯åº¦
        ssl_target = XV - XK  # è‡ªç›£ç£å­¸ç¿’ç›®æ¨™
        grad_loss = ttt_norm_out - ssl_target
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šæ¢¯åº¦è£å‰ª
        grad_loss = torch.clamp(grad_loss, min=-5.0, max=5.0)
        
        # 4. ç°¡åŒ–çš„åƒæ•¸æ›´æ–° (å–®æ¬¡æ›´æ–°)
        eta = ttt_lr * ttt_lr_mult
        eta = torch.clamp(eta, min=1e-6, max=0.1)  # é™åˆ¶å­¸ç¿’ç‡
        
        # è¨ˆç®—æ¢¯åº¦
        grad_W1 = torch.zeros_like(W1_init)
        grad_b1 = torch.zeros_like(b1_init)
        
        # ç°¡åŒ–çš„æ¢¯åº¦è¨ˆç®— (åªä½¿ç”¨æœ€å¾Œå¹¾å€‹ token)
        last_tokens = min(16, N)  # åªä½¿ç”¨æœ€å¾Œ 16 å€‹ token
        for j in range(N - last_tokens, N):
            grad_W1 += eta[:, j:j+1, :] * XK[:, j:j+1, :].transpose(-2, -1) @ grad_loss[:, j:j+1, :]
            grad_b1 += eta[:, j:j+1, :] * grad_loss[:, j:j+1, :]
        
        # æ›´æ–°åƒæ•¸
        W1_updated = W1_init - grad_W1
        b1_updated = b1_init - grad_b1
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šåƒæ•¸ç¯„åœé™åˆ¶
        W1_updated = torch.clamp(W1_updated, min=-2.0, max=2.0)
        b1_updated = torch.clamp(b1_updated, min=-2.0, max=2.0)
        
        # 5. è¨ˆç®—è¼¸å‡º
        Z1_updated = XQ @ W1_updated + b1_updated
        
        # å†æ¬¡æ‡‰ç”¨ LayerNorm
        mu_updated = Z1_updated.mean(dim=-1, keepdim=True)
        var_updated = Z1_updated.var(dim=-1, keepdim=True, unbiased=False)
        std_updated = torch.sqrt(var_updated + 1e-5)
        Z1_hat_updated = (Z1_updated - mu_updated) / std_updated
        
        Z1_normed_updated = (self.ttt_norm_weight * Z1_hat_updated.reshape(-1, NH, N, HF) + 
                             self.ttt_norm_bias).reshape(B_mul_NH, N, HF)
        
        # 6. æ®˜å·®é€£æ¥: f(x) = x + LN(f_res(x))
        output = XQ + Z1_normed_updated  # [B*nh, N, f]
        
        # ğŸ”§ ç©©å®šæ€§æ”¹é€²ï¼šé™åˆ¶æœ€çµ‚è¼¸å‡ºç¯„åœ
        output = torch.clamp(output, min=-10.0, max=10.0)
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        output = output.reshape(batch_size, self.num_heads, seq_length, self.head_dim)
        output = output.permute(0, 2, 1, 3).reshape(batch_size, seq_length, -1)
        
        # æ‡‰ç”¨é–€æ§ - èˆ‡ kernels ç‰ˆæœ¬ä¸€è‡´ä½¿ç”¨ GELU
        gate = F.gelu(XGate)
        output = gate * self.post_norm(output)
        
        # æœ€çµ‚æŠ•å½±
        output = self.wo(output)
        
        return output
