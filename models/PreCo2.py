import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any
from dataclasses import dataclass, field
import math

from .longhorn import LonghornLM, LonghornConfig
from .ttt import TTT, TTTConfig


@dataclass
class PreCoConfig:
    """PreCo (Prediction-Correction) æ··åˆæ¨¡å‹é…ç½® - 133M åƒæ•¸é‡åŒ¹é…ç‰ˆæœ¬"""
    # Longhorn é…ç½® - åƒæ•¸é‡å„ªåŒ–
    longhorn_d_model: int = 512   # 512 ç¶­åº¦
    longhorn_n_layer: int = 8     # 8 å±¤ (èˆ‡ run.sh ä¸€è‡´)
    longhorn_d_state: int = 4     # èˆ‡ Longhorn ä¿æŒä¸€è‡´
    longhorn_ssm_expand: int = 6  # èˆ‡ run.sh ä¸€è‡´ï¼šexpand=8
    
    # TTT é…ç½® - åƒæ•¸é‡å„ªåŒ–
    ttt_hidden_size: int = 512    # 512 ç¶­åº¦
    ttt_num_layers: int = 6      # 6 å±¤ (èˆ‡ run.sh ä¸€è‡´)
    ttt_num_heads: int = 8        # 8 é ­ (512/8=64 head_dim)
    ttt_base_lr: float = 1.0
    ttt_mini_batch_size: int = 8  # åŒ¹é…æ³¨æ„åŠ›é ­æ•¸
    mlp_ratio: int = 2            # MLP å€æ•¸ (èˆ‡ run.sh ä¸€è‡´)
    
    # Kalman Gain é…ç½® - å¢å¼·å‹•æ…‹æ€§
    kalman_hidden_dim: int = 256  # 256 ç¶­ (èˆ‡ run.sh ä¸€è‡´)
    
    # å…±åŒé…ç½®
    vocab_size: int = 50257       # åŒ¹é… slim_tokenizer
    block_size: int = 1024        # åŒ¹é… run.sh
    dropout: float = 0.2
    
    # TTT ç‰¹å®šé…ç½®
    use_gate: bool = True         # å•Ÿç”¨é–€æ§æ©Ÿåˆ¶
    share_qk: bool = True         # å•Ÿç”¨ Q/K å…±äº«
    ttt_layer_type: str = "linear"
    pre_conv: bool = True         # å•Ÿç”¨é å·ç©
    conv_kernel: int = 2          # å·ç©æ ¸å¤§å°
    scan_checkpoint_group_size: int = 1
    
    def __post_init__(self):
        """åˆå§‹åŒ–å¾Œè¨­ç½® SSM é…ç½®"""
        self.longhorn_ssm_cfg = {
            'd_state': self.longhorn_d_state,
            'd_conv': 3,
            'expand': self.longhorn_ssm_expand
        }
    
    # ğŸ¯ 133M åƒæ•¸åˆ†ä½ˆé ä¼° (åƒæ•¸é‡åŒ¹é…ç‰ˆæœ¬)ï¼š
    #
    # ğŸ”§ å„ªåŒ–å¾Œçš„åƒæ•¸åˆ†ä½ˆ (èˆ‡ Longhorn 133M åŒ¹é…)ï¼š
    # - å…±ç”¨ Embedding: 512Ã—50257 â‰ˆ 26M (Longhorn å’Œ TTT å…±ç”¨åŒä¸€å€‹)
    # - Longhorn backbone: 10å±¤Ã—512ç¶­Ã—expand8 â‰ˆ 42M (å¢åŠ å®¹é‡)
    # - TTT backbone: 6å±¤Ã—512ç¶­Ã—8é ­ â‰ˆ 32M (å¹³è¡¡åˆ†é…)
    # - å”¯ä¸€çš„ q LM Head: 512Ã—50257 â‰ˆ 26M
    # - Kalman Gain: 256ç¶­ â‰ˆ 0.7M (å¢å¼·å‹•æ…‹æ€§)
    # - å…¶ä»–çµ„ä»¶: ~6M
    # - ç¸½è¨ˆï¼šç´„133Måƒæ•¸ (èˆ‡ Longhorn åŒ¹é…)
    #
    # ğŸš€ é—œéµå„ªåŒ–ï¼š
    # - Longhorn å±¤æ•¸ï¼šå¾8å±¤å¢åŠ åˆ°10å±¤ï¼Œå¢åŠ ç´„10Måƒæ•¸
    # - Longhorn expandï¼šå¾6å¢åŠ åˆ°8ï¼Œå¢åŠ å…§éƒ¨å®¹é‡
    # - TTT å±¤æ•¸ï¼šèª¿æ•´åˆ°6å±¤ï¼Œå¹³è¡¡å…©å€‹ backbone
    # - Kalman Gainï¼šç¶­æŒ256ç¶­ï¼Œå¢å¼·å‹•æ…‹åˆ†é…èƒ½åŠ›


class KalmanGainNetwork(nn.Module):
    """é«˜æ•ˆç‰ˆ Kalman Gate - ä¿æŒå‹•æ…‹æ ¡æ­£èƒ½åŠ›"""
    def __init__(self, d_model: int, hidden_dim: int):
        super().__init__()
        
        # ä½¿ç”¨å–®å±¤ç·šæ€§è®Šæ› + LayerNorm
        self.norm = nn.LayerNorm(d_model)
        self.score = nn.Linear(d_model, 1)
        
        # åˆå§‹åŒ–ï¼šè®“åˆå§‹æ¬Šé‡æ¥è¿‘ 0.5
        with torch.no_grad():
            self.score.weight.data.normal_(0, 0.02)
            self.score.bias.data.fill_(0.0)
    
    def forward(self, hidden_states):
        # å¿«é€Ÿè¨ˆç®—é‡è¦æ€§åˆ†æ•¸
        normed = self.norm(hidden_states)
        scores = self.score(normed).squeeze(-1)  # [B, L]
        kalman_weights = torch.sigmoid(scores)    # [B, L]
        
        # çµ±è¨ˆä¿¡æ¯
        token_stats = {
            'mean': kalman_weights.mean().item(),
            'std': kalman_weights.std().item(),
        }
        return kalman_weights, token_stats


class PreCoModel(nn.Module):
    """PreCo æ¨¡å‹ - é«˜æ•ˆç‰ˆæœ¬ä½†ä¿æŒ Kalman Filter é‚è¼¯"""
    
    def __init__(self, config: PreCoConfig):
        super().__init__()
        
        # Longhorn é æ¸¬å™¨
        longhorn_config = LonghornConfig()
        longhorn_config.vocab_size = config.vocab_size
        longhorn_config.d_model = config.longhorn_d_model
        longhorn_config.n_layer = config.longhorn_n_layer
        longhorn_config.ssm_cfg = {
            'd_state': config.longhorn_d_state,
            'd_conv': 3,
            'expand': config.longhorn_ssm_expand
        }
        self.longhorn = LonghornLM(longhorn_config)
        
        # TTT æ ¡æ­£å™¨
        self.ttt = TTT(TTTConfig(
            vocab_size=config.vocab_size,
            hidden_size=config.ttt_hidden_size,
            num_hidden_layers=config.ttt_num_layers,
            num_attention_heads=config.ttt_num_heads,
            max_position_embeddings=config.block_size,
            ttt_base_lr=config.ttt_base_lr,
            mini_batch_size=config.ttt_mini_batch_size,
            use_gate=config.use_gate,
            ttt_layer_type=config.ttt_layer_type,
            scan_checkpoint_group_size=config.scan_checkpoint_group_size,
            pre_conv=config.pre_conv,
            conv_kernel=config.conv_kernel,
            pad_token_id=0,
            bos_token_id=2,
            eos_token_id=3,
        ))
        
        # Kalman Gate
        self.kalman_gain = KalmanGainNetwork(
            d_model=config.longhorn_d_model,
            hidden_dim=config.kalman_hidden_dim,
        )
        
        # å…±ç”¨ Q Network
        self.q_network = nn.Linear(config.longhorn_d_model, config.vocab_size, bias=False)
        nn.init.normal_(self.q_network.weight, std=0.02)
        
        # correction scale
        self.correction_scale = nn.Parameter(torch.ones(1) * 0.2)
    
    def forward(self, input_ids, target_ids=None, ttt_lr_mult=1.0):
        # 1. Longhorn é æ¸¬
        longhorn_logits, longhorn_loss, longhorn_hidden = self.longhorn(
            input_ids, targets=target_ids, return_hidden=True
        )
        
        # 2. TTT æ ¡æ­£
        ttt_logits, ttt_loss, ttt_hidden = self.ttt(
            input_ids=input_ids,
            labels=target_ids,
            ttt_lr_mult=ttt_lr_mult,
            return_hidden=True
        )
        
        # 3. Kalman Gate - è¨ˆç®—æ¯å€‹ token çš„é‡è¦æ€§
        kalman_weights, token_stats = self.kalman_gain(longhorn_hidden)  # [B, L]
        
        # 4. è¨ˆç®— Q å€¼
        q_longhorn = self.q_network(longhorn_hidden)  # [B, L, V]
        q_ttt = self.q_network(ttt_hidden)           # [B, L, V]
        
        # 5. Kalman Filter æ›´æ–° - æ ¹æ“šé‡è¦æ€§å‹•æ…‹æ ¡æ­£
        correction = q_ttt - q_longhorn              # [B, L, V]
        scale = torch.sigmoid(self.correction_scale)  # scalar
        final_logits = q_longhorn + scale * kalman_weights.unsqueeze(-1) * correction
        
        if target_ids is not None:
            # è¨ˆç®— CE loss
            loss = F.cross_entropy(
                final_logits.view(-1, final_logits.size(-1)), 
                target_ids.view(-1)
            )
            
            # è¿”å›å¿…è¦çš„çµ±è¨ˆä¿¡æ¯
            loss_dict = {
                'total_loss': loss,
                'ce_loss': loss.item(),
                'longhorn_loss': longhorn_loss.item(),
                'ttt_loss': ttt_loss.item(),
                'kalman_mean': token_stats['mean'],
                'kalman_std': token_stats['std'],
            }
            return final_logits, loss_dict
        return final_logits


# å·¥å» å‡½æ•¸ - Kalman Filter ç‰ˆæœ¬
def create_preco_model(
    vocab_size: int = 50257,     # åŒ¹é… slim tokenizer
    block_size: int = 1024,      # ğŸ”§ æ›´æ–°ï¼šåŒ¹é… run.sh
    longhorn_d_model: int = 512, # 512ç¶­åº¦å„ªåŒ–ï¼š512ç¶­åº¦
    longhorn_n_layer: int = 10,  # ğŸ”§ æ›´æ–°ï¼š10å±¤ (åƒæ•¸é‡åŒ¹é…)
    ttt_hidden_size: int = 512,  # 512ç¶­åº¦å„ªåŒ–ï¼š512ç¶­åº¦
    ttt_num_layers: int = 6,     # ğŸ”§ æ›´æ–°ï¼š6å±¤ (åƒæ•¸é‡å¹³è¡¡)
    ttt_num_heads: int = 8,      # 512ç¶­åº¦å„ªåŒ–ï¼š8é ­ (512/8=64)
    longhorn_ssm_expand: int = 8, # ğŸ”§ æ–°å¢ï¼šexpand=8 (å¢åŠ å®¹é‡)
    kalman_hidden_dim: int = 256, # ğŸ”§ æ–°å¢ï¼š256ç¶­ (å¢å¼·å‹•æ…‹æ€§)
    **kwargs
) -> PreCoModel:
    """å‰µå»º PreCo Kalman Filter æ¨¡å‹ - 133M åƒæ•¸é‡åŒ¹é…ç‰ˆæœ¬"""
    config = PreCoConfig(
        vocab_size=vocab_size,
        block_size=block_size,
        longhorn_d_model=longhorn_d_model,
        longhorn_n_layer=longhorn_n_layer,
        longhorn_ssm_expand=longhorn_ssm_expand,  # ğŸ”§ æ–°å¢
        ttt_hidden_size=ttt_hidden_size,
        ttt_num_layers=ttt_num_layers,
        ttt_num_heads=ttt_num_heads,
        kalman_hidden_dim=kalman_hidden_dim,      # ğŸ”§ æ–°å¢
        **kwargs
    )
    return PreCoModel(config) 