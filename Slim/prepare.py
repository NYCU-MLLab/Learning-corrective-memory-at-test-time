import os
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from tokenizers import Tokenizer
from transformers import PreTrainedTokenizerFast
import pickle
import math

num_proc = 10 # å¯æ ¹æ“š CPU æ ¸å¿ƒèª¿æ•´(æŒ‡ä»¤:nproc)

if __name__ == "__main__":
    # === Step 1: åªè¼‰å…¥ train å’Œ val è³‡æ–™ ===
    root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
    data_files = {
        "train": os.path.join(root, "dataset/SlimPajama-1B/SlimPajama-1B_train.jsonl"),
        "val": os.path.join(root, "dataset/SlimPajama-1B/SlimPajama-1B_validation.jsonl"),
    }
    dataset = load_dataset("json", data_files=data_files)

    # === Step 2: è¼‰å…¥ä½ è‡ªå·±çš„ tokenizer ===
    raw_tokenizer = Tokenizer.from_file("/root/Thesis/tokenizer/slim_tokenizer/tokenizer.json")
    print("raw_tokenizer:", raw_tokenizer)
    tokenizer = PreTrainedTokenizerFast(tokenizer_object=raw_tokenizer)
    tokenizer.add_special_tokens({
        "eos_token": "</s>",
        "bos_token": "<s>",
        "pad_token": "<pad>",
        "unk_token": "<unk>"
    })
    print("Special tokens:", tokenizer.special_tokens_map)
    print(tokenizer.all_special_tokens)
    print(tokenizer.vocab_size)
    
    # é©—è­‰token IDæ˜ å°„
    print(f"pad_token_id: {tokenizer.pad_token_id}")
    print(f"bos_token_id: {tokenizer.bos_token_id}")
    print(f"eos_token_id: {tokenizer.eos_token_id}")
    print(f"unk_token_id: {tokenizer.unk_token_id}")
    
    # é©—è­‰èˆ‡TTTé…ç½®çš„ä¸€è‡´æ€§
    expected_mapping = {
        "pad_token_id": 0,
        "bos_token_id": 2, 
        "eos_token_id": 3,
        "unk_token_id": 1
    }
    
    actual_mapping = {
        "pad_token_id": tokenizer.pad_token_id,
        "bos_token_id": tokenizer.bos_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "unk_token_id": tokenizer.unk_token_id
    }
    
    for key, expected in expected_mapping.items():
        actual = actual_mapping[key]
        if actual != expected:
            print(f"âš ï¸ è­¦å‘Š: {key} = {actual}, æœŸæœ› = {expected}")
        else:
            print(f"âœ… {key} = {actual} (æ­£ç¢º)")
    
    eos_id = tokenizer.eos_token_id
    if eos_id is None:
        raise ValueError("Tokenizer is missing eos_token_id. Please check tokenizer training.")
    print("eos_id:", eos_id)

    # === Step 3: å®šç¾© tokenization è™•ç†å‡½å¼ ===
    def process(example):
        text = example["text"].strip()
        ids = tokenizer.encode(text, add_special_tokens=False)
        ids.append(eos_id)
        return {"ids": ids, "len": len(ids)}

    # === Step 4: å° train å’Œ val åš tokenize ===
    tokenized = dataset.map(
        process,
        remove_columns=["text"],
        desc="Tokenizing slim splits",
        num_proc=num_proc
    )

    # === Step 5: å„²å­˜æˆJAXé¢¨æ ¼çš„é•·é™£åˆ—æ ¼å¼ ===
    # ğŸ”§ ä¿®æ”¹ï¼šç§»é™¤é åˆ‡åˆ†ï¼Œåªä¿å­˜é•·é™£åˆ—
    block_size = 2048  # åŒ¹é…JAX mainå’Œè¨“ç·´é…ç½®
    for split in ["train", "val"]:
        dset = tokenized[split]
        arr_len = np.sum(dset["len"], dtype=np.uint64)
        filename = f"{split}.bin"  # åªä¿å­˜é•·é™£åˆ—
        dtype = np.uint16

        if tokenizer.vocab_size >= 2**16:
            raise ValueError("Tokenizer vocab too large for uint16. Use uint32 instead.")

        # å‰µå»º memmap æª”æ¡ˆ - åªä¿å­˜é•·é™£åˆ—
        arr = np.memmap(filename, dtype=dtype, mode="w+", shape=(arr_len,))
        total_batches = 128

        # å¯«å…¥ tokenized è³‡æ–™
        idx = 0
        for batch_idx in tqdm(range(total_batches), desc=f"Writing {filename}"):
            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format("numpy")
            arr_batch = np.concatenate(batch["ids"])
            arr[idx : idx + len(arr_batch)] = arr_batch
            idx += len(arr_batch)
        arr.flush()
        
        print(f"âœ… {split} é•·é™£åˆ—å·²ä¿å­˜: {filename}")
        print(f"  ç¸½tokens: {arr_len:,}")
        print(f"  é ä¼°åºåˆ—æ•¸: {arr_len // block_size:,} (æ¯åºåˆ—{block_size} tokens)")

    # === Step 6: å»ºç«‹ meta.pkl ===
    meta = {
        "vocab_size": tokenizer.vocab_size,
        "block_size": block_size
    }
    with open("meta.pkl", "wb") as f:
        pickle.dump(meta, f)

    print(f"âœ… JAXé¢¨æ ¼è³‡æ–™æ ¼å¼å·²ç”Ÿæˆ:")
    print(f"  - train.bin: é•·é™£åˆ—æ ¼å¼")
    print(f"  - val.bin: é•·é™£åˆ—æ ¼å¼") 
    print(f"  - meta.pkl: åŒ…å«vocab_size={tokenizer.vocab_size}, block_size={block_size}")
    print(f"  - æ”¯æ´æ»‘å‹•çª—å£åˆ‡åˆ†ï¼Œèˆ‡JAX mainä¸€è‡´")